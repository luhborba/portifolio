{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"P\u00e1gina Inicial [Projeto Ainda em Constru\u00e7\u00e3o] Seja bem-vindo(a) ao meu universo de dados e projetos! Me chamo Luciano Borba, est\u00e1 p\u00e1gina \u00e9 uma forma para que voc\u00eas me conhe\u00e7am melhor, n\u00e3o apenas como profissional, mas tamb\u00e9m como pessoa. A ideia de navega\u00e7\u00e3o, \u00e9 que voc\u00ea possa visualizar de forma categorizada alguns pontos, s\u00e3o eles: P\u00e1gina Inicial: P\u00e1gina de Sauda\u00e7\u00e3o, tamb\u00e9m conhecida como p\u00e1gina atual; Sobre: Contendo informa\u00e7\u00f5es gerais sobre mim, curiosidades e afins; Vida Acad\u00eamica: Contendo Informa\u00e7\u00f5es gerais sobre minha vida de estudos/cursos; Experi\u00eancias: Contendo Informa\u00e7\u00f5es sobre minhas experiencias profissionais; Habilidades: Contendo minhas principais habilidades; Canal Youtube: Falando um pouco sobre o trabalho que desenvolvo na produ\u00e7\u00e3o de conte\u00fado no Youtube; Projetos: Aqui voc\u00ea conseguira acessar meus principais projetos, e acompanhar passo-a-passo minhas tomadas de decis\u00e3o; Se conecte comigo:","title":"P\u00e1gina Inicial"},{"location":"#pagina-inicial","text":"[Projeto Ainda em Constru\u00e7\u00e3o] Seja bem-vindo(a) ao meu universo de dados e projetos! Me chamo Luciano Borba, est\u00e1 p\u00e1gina \u00e9 uma forma para que voc\u00eas me conhe\u00e7am melhor, n\u00e3o apenas como profissional, mas tamb\u00e9m como pessoa. A ideia de navega\u00e7\u00e3o, \u00e9 que voc\u00ea possa visualizar de forma categorizada alguns pontos, s\u00e3o eles: P\u00e1gina Inicial: P\u00e1gina de Sauda\u00e7\u00e3o, tamb\u00e9m conhecida como p\u00e1gina atual; Sobre: Contendo informa\u00e7\u00f5es gerais sobre mim, curiosidades e afins; Vida Acad\u00eamica: Contendo Informa\u00e7\u00f5es gerais sobre minha vida de estudos/cursos; Experi\u00eancias: Contendo Informa\u00e7\u00f5es sobre minhas experiencias profissionais; Habilidades: Contendo minhas principais habilidades; Canal Youtube: Falando um pouco sobre o trabalho que desenvolvo na produ\u00e7\u00e3o de conte\u00fado no Youtube; Projetos: Aqui voc\u00ea conseguira acessar meus principais projetos, e acompanhar passo-a-passo minhas tomadas de decis\u00e3o;","title":"P\u00e1gina Inicial"},{"location":"p2-sobre/","text":"Sobre Eu sou Luciano Borba, um entusiasta da engenharia anal\u00edtica e apaixonado por desafios no mundo dos dados. Atualmente, desempenho o papel de Gerente de Projetos e Dados, onde combino minha experi\u00eancia t\u00e9cnica com habilidades de lideran\u00e7a para impulsionar equipes e atingir resultados excepcionais. Sempre gostei do mundo de Gest\u00e3o, de avaliar metricas e trabalhar com prazos e metas, por\u00e9m logo ap\u00f3s me formar entendi que n\u00e3o existia a possibilidade de me tornar Gestor logo de cara, como tamb\u00e9m passei por v\u00e1rias situa\u00e7\u00f5es ao longo da vida, dentros da oportunidades que tive uma dela foi dar suporte a sistemas, vez ou outra algum gestor n\u00e3o se sentia satisfeito com os relat\u00f3rios j\u00e1 presentes no sistema, ent\u00e3o eu acessava o banco para criar uma consulta que atendesse tal situa\u00e7\u00e3o. Assim come\u00e7ou minha vida em dados, simplesmente me apaixonei pela sensa\u00e7\u00e3o de trabalhar com dados para gerar valor para a minha organiza\u00e7\u00e3o como um todo, foi um caminho sem volta. Minha primeira experi\u00eancia foi com sistemas com Banco SQL Server e Oracle Database. Dai partir para o estudo de PL/SQL me aprofundando um pouco mais no mundo Oracle, tive minha primeira experi\u00eancia tamb\u00e9m com Oracle Apex. Ai foi sem freio, Python, Power BI, Pentaho, entre outras ferramentas, fui estudando sem parar e continuo assim, acredito que pessoas mentem, mas os n\u00fameros falam por si s\u00f3, ent\u00e3o trabalhar com dados \u00e9 extremamente valioso para mim, algo que acredito que pode mudar qualquer cen\u00e1rio. Minha jornada \u00e9 ainda mais enriquecida pela paternidade, sendo pai de tr\u00eas incr\u00edveis filhos (Maria Clara, Gabriel e Maria J\u00falia), sendo um deles uma fonte constante de aprendizado e inspira\u00e7\u00e3o, por ser uma crian\u00e7a autista (Gabriel). Essa experi\u00eancia moldou minha abordagem \u00e0 vida, destacando a import\u00e2ncia da empatia, paci\u00eancia e compreens\u00e3o. Al\u00e9m de ser casado com uma mulher incr\u00edvel chamada Raquel Barreto, psic\u00f3loga cl\u00ednica. Al\u00e9m do meu lado profissional e familiar, sou tamb\u00e9m um gamer \u00e1vido (j\u00e1 tive bastante vontade de streamar), mergulhado nos universos dos jogos que, de certa forma, me ensinaram a abordar problemas com criatividade e estrat\u00e9gia. Os jogos que atualmente mais jogo s\u00e3o: League Of Legends Far Cry 6 Meus jogos prediletos s\u00e3o: League of Legends Assassins Creed (Todos, inclusive fiz a leitura dos livros) Neverwinter Nights 1 e 2 (Raiz) Need For Speed Underground 1 e 2 Need For Speed Most Wanted Need For Speed Carbon Ragnarok Lineage 2 Super Mario World Mario Kart Street Fight II Pro Evolution Soccer Sou torcedor do Flamengo e Botafogo-PB, hoje n\u00e3o acompanho tanto mais o futebol, devido a tempo e prioridade, como tamb\u00e9m n\u00e3o consigo ter mais acessos aos jogos de forma t\u00e3o constante. Gosto bastante de competi\u00e7\u00f5es. Neste espa\u00e7o digital, compartilho n\u00e3o apenas minhas realiza\u00e7\u00f5es profissionais, mas tamb\u00e9m um pouco da minha ess\u00eancia como indiv\u00edduo. Espero que essa visita proporcione uma vis\u00e3o mais completa de quem sou, tanto como profissional de dados quanto como pessoa. Sinta-se \u00e0 vontade para explorar e descobrir mais sobre mim e meu trabalho. Se conecte comigo:","title":"Sobre"},{"location":"p2-sobre/#sobre","text":"Eu sou Luciano Borba, um entusiasta da engenharia anal\u00edtica e apaixonado por desafios no mundo dos dados. Atualmente, desempenho o papel de Gerente de Projetos e Dados, onde combino minha experi\u00eancia t\u00e9cnica com habilidades de lideran\u00e7a para impulsionar equipes e atingir resultados excepcionais. Sempre gostei do mundo de Gest\u00e3o, de avaliar metricas e trabalhar com prazos e metas, por\u00e9m logo ap\u00f3s me formar entendi que n\u00e3o existia a possibilidade de me tornar Gestor logo de cara, como tamb\u00e9m passei por v\u00e1rias situa\u00e7\u00f5es ao longo da vida, dentros da oportunidades que tive uma dela foi dar suporte a sistemas, vez ou outra algum gestor n\u00e3o se sentia satisfeito com os relat\u00f3rios j\u00e1 presentes no sistema, ent\u00e3o eu acessava o banco para criar uma consulta que atendesse tal situa\u00e7\u00e3o. Assim come\u00e7ou minha vida em dados, simplesmente me apaixonei pela sensa\u00e7\u00e3o de trabalhar com dados para gerar valor para a minha organiza\u00e7\u00e3o como um todo, foi um caminho sem volta. Minha primeira experi\u00eancia foi com sistemas com Banco SQL Server e Oracle Database. Dai partir para o estudo de PL/SQL me aprofundando um pouco mais no mundo Oracle, tive minha primeira experi\u00eancia tamb\u00e9m com Oracle Apex. Ai foi sem freio, Python, Power BI, Pentaho, entre outras ferramentas, fui estudando sem parar e continuo assim, acredito que pessoas mentem, mas os n\u00fameros falam por si s\u00f3, ent\u00e3o trabalhar com dados \u00e9 extremamente valioso para mim, algo que acredito que pode mudar qualquer cen\u00e1rio. Minha jornada \u00e9 ainda mais enriquecida pela paternidade, sendo pai de tr\u00eas incr\u00edveis filhos (Maria Clara, Gabriel e Maria J\u00falia), sendo um deles uma fonte constante de aprendizado e inspira\u00e7\u00e3o, por ser uma crian\u00e7a autista (Gabriel). Essa experi\u00eancia moldou minha abordagem \u00e0 vida, destacando a import\u00e2ncia da empatia, paci\u00eancia e compreens\u00e3o. Al\u00e9m de ser casado com uma mulher incr\u00edvel chamada Raquel Barreto, psic\u00f3loga cl\u00ednica. Al\u00e9m do meu lado profissional e familiar, sou tamb\u00e9m um gamer \u00e1vido (j\u00e1 tive bastante vontade de streamar), mergulhado nos universos dos jogos que, de certa forma, me ensinaram a abordar problemas com criatividade e estrat\u00e9gia. Os jogos que atualmente mais jogo s\u00e3o: League Of Legends Far Cry 6 Meus jogos prediletos s\u00e3o: League of Legends Assassins Creed (Todos, inclusive fiz a leitura dos livros) Neverwinter Nights 1 e 2 (Raiz) Need For Speed Underground 1 e 2 Need For Speed Most Wanted Need For Speed Carbon Ragnarok Lineage 2 Super Mario World Mario Kart Street Fight II Pro Evolution Soccer Sou torcedor do Flamengo e Botafogo-PB, hoje n\u00e3o acompanho tanto mais o futebol, devido a tempo e prioridade, como tamb\u00e9m n\u00e3o consigo ter mais acessos aos jogos de forma t\u00e3o constante. Gosto bastante de competi\u00e7\u00f5es. Neste espa\u00e7o digital, compartilho n\u00e3o apenas minhas realiza\u00e7\u00f5es profissionais, mas tamb\u00e9m um pouco da minha ess\u00eancia como indiv\u00edduo. Espero que essa visita proporcione uma vis\u00e3o mais completa de quem sou, tanto como profissional de dados quanto como pessoa. Sinta-se \u00e0 vontade para explorar e descobrir mais sobre mim e meu trabalho.","title":"Sobre"},{"location":"p3-academica/","text":"Vida Acad\u00eamica Como amante do estudo, a vida acad\u00eamica \u00e9 algo que sempre estar\u00e1 em atualiza\u00e7\u00e3o, pois como profissional de tecnologia sei que sempre estarei estudando, pois minha \u00e1rea est\u00e1 em constante evolu\u00e7\u00e3o e que assim eu possa evoluir junto. Est\u00e1 p\u00e1gina ira conter todas as minhas forma\u00e7\u00f5es acad\u00eamicas, como tamb\u00e9m todas as minhas certifica\u00e7\u00f5es e cursos livres, conte\u00fados esses que me constru\u00edram e me ajudaram a subir de n\u00edvel na minha carreira, como tamb\u00e9m a resolver diversos problemas dentro da minha organiza\u00e7\u00e3o. Forma\u00e7\u00e3o Acad\u00eamica MBA em Business Intelligence na Faculdade Descomplica [Mar\u00e7o/2023] P\u00f3s-Graduado em An\u00e1lise de Dados na Faculdade Descomplica [Mar\u00e7o/2023] Tecn\u00f3logo em Gest\u00e3o da Tecnologia da Informa\u00e7\u00e3o na Faculdade Internacional da Para\u00edba-FPB [Junho/2014] Certifica\u00e7\u00f5es Python (Basic) HackerRank [Novembro/2023] ISO 37001 (Antisuborno) e ISO 37301 (Complice) - Tradius [Outubro/2023] Business Intelligence Foundation Professional Certification BIFPC [Junho/2023] Cursos Livres Engenharia de Dados Workshop 2 - Jornada de Dados 2024 - Pydantic, DataQuality e TDD para dados - 4 Horas - Luciano Galv\u00e3o [Fevereiro/2024] Workshop Data Science - Databricks, Spark e WebScraping - 4 Horas - Data Viking [Janeiro/2024] Databricks: Trabalhando com Diversos Formatos e Tipos de Arquivos - 8 Horas - Alura [Dezembro/2023] Databricks: Conhecendo a Ferramenta - 8 Horas - Alura [Novembro/2023] Spark: Apresentando a Ferramenta - 10 Horas - Alura [Setembro/2023] Forma\u00e7\u00e3o BI e Data Warehouse com SQL Server e Power BI - 6 Cursos - 60 Horas - Alura [Julho/2023] Transforma\u00e7\u00e3o com ETL: Pentaho Data Integration - 16 Horas - Alura [Maio/2023] Modelos de ETL: Pentaho Data Integration - 14 Horas - Alura [Abril/2023] Business Intelligence: Trabalhando com Data Warehouse - 12 Horas - Alura [Abril/2023] Python para Engenheiros e Cientistas / B\u00e1sico ao Avan\u00e7ado - 19 Horas - Udemy - Rafael Pereira [Maio/2022] An\u00e1lise de Dados Pandas: Conhecendo a Biblioteca - 8 Horas - Alura [Dezembro/2023] Estat\u00edstica com Python: Frequ\u00eancias e Medidas - 10 Horas - Alura [Novembro/2023] Data Visualization com Python: biblioteca Plotly - 5 Horas - Udemy - Data Viking [Outubro/2023] Data Science: Analise e Visualiza\u00e7\u00e3o de dados - 6 Horas - Alura [Maio/2023] Python para Data Science: Fun\u00e7\u00f5es, Pacotes e Pandas - 10 Horas - Alura [Maio/2023] Python para Data Science: Linguagem e Numpy - 12 Horas - Alura [Maio/2023] SQL para An\u00e1lise de Dados - 11 Horas - Udemy - Caio Avelino [Setembro/2022] Introdu\u00e7\u00e3o a Tableau - 5 Horas - EIA - Fernando Amaral [Abril/2022] Microsoft Power BI para Data Science 2.0 - 72 Horas - Data Science Academy [Abril/2022] Visualiza\u00e7\u00e3o de Dados (DataViz) - 10 Horas - Souk Analytics - Tiago Marques [Abril/2022] Python para Iniciantes: Aprenda do Zero - 5 Horas - EIA - Fernando Amaral [Abril/2022] Python para An\u00e1lise de Dados - 12 Horas - Udemy - Data Viking [Abril/2022] Dashboards Matadores: Domine a Arte de Apresenta\u00e7\u00e3o de Dados - 3 Horas - EIA - Fernando Amaral [Mar\u00e7o/2022] Data Science para Iniciantes: F\u00e1cil e Simples - 3 Horas - EIA - Fernando Amaral [Mar\u00e7o/2022] Python Fundamentos para An\u00e1lise de Dados 3.0 - 60 Horas - Data Science Academy [Mar\u00e7o/2022] Data Science: Visualiza\u00e7\u00e3o de dados com Python - 2 Horas - Diego Mariano [Fevereiro/2022] Power BI Completo - 9 Horas - Udemy - Jo\u00e3o Paulo [Fevereiro/2021] Governan\u00e7a: Governan\u00e7a de TI: Gest\u00e3o de Requisitos no Contexto \u00c1gil - 8 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Gest\u00e3o de Requisitos no Contexto \u00c1gil - 8 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Gest\u00e3o de Programas, Projetos e Produtos - 8 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Gerencie Servi\u00e7os e Fortale\u00e7a a Seguran\u00e7a - 6 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Fundamentos da Gest\u00e3o de Portif\u00f3lios - 6 Horas - Alura [Janeiro/2024] Governan\u00e7a de TI: Modelo de Gest\u00e3o, Arquitetura e Inova\u00e7\u00e3o - 8 Horas - Alura [Janeiro/2024] Governan\u00e7a de TI: Alinhamento Estrat\u00e9gico - 8 Horas - Alura [Janeiro/2024] LGPD Registro das opera\u00e7\u00f5es de tratamento de Dados Pessoais - 3 Horas - Udemy - Cl\u00e1udio Dodt [Mar\u00e7o/2022] LGPD Zero: Conceitos fundamentais de Prote\u00e7\u00e3o Dados Pessoais - 1 Hora - Udemy - Cl\u00e1udio Dodt [Mar\u00e7o/2022] LGPD Pol\u00edtica de Privacidade e Prote\u00e7\u00e3o de Dados Pessoais - 4 Horas - Udemy - Cl\u00e1udio Dodt [Janeiro/2022] Fundamentos do Scrum - 1,5 Horas - Udemy - Pilar Sanchez [Janeiro/2021] Dados Conceitos Gerais sobre Dados: O que voc\u00ea precisa Saber - 5 Horas - Udemy - Grimaldo Oliveira [Janeiro/2024] Modelagem de Banco de Dados Relacional: Normaliza\u00e7\u00e3o - 8 Horas - Alura [Novembro/2023] Modelagem de Banco de Dados Relacional: \u00c1lgebra Relacional - 8 Horas - Alura [Novembro/2023] Redes Neurais: Deep Learning com PyTorch - 6 Horas - Alura [Novembro/2023] PostgreSQL: Comandos DML e DDL - 8 Horas - Alura [Setembro/2023] PostgreSQL: Views, Sub-Consultas e Fun\u00e7\u00f5es - 6 Horas - Alura [Setembro/2023] PostgreSQL - 8 Horas - Alura [Julho/2023] Forma\u00e7\u00e3o Modelagem de Dados - 4 Cursos - 33 Horas - Alura [Julho/2023] Forma\u00e7\u00e3o Power BI - 5 Cursos - 55 Horas - Alura [Junho/2023] SQL com Python - 3 Horas - Souk Analytics - Data Viking [Abril/2022] Fundamentos do BigData T\u00e9cnicas e Conceitos - 2 Horas - LinkedIn Learning [Fevereiro/2022] Big Data Fundamentos 3.0 - 12 Horas - Data Science Academy [Fevereiro/2022] Python para Ci\u00eancia de Dados: Forma\u00e7\u00e3o B\u00e1sica - 4 Horas - LinkedIn Learning [Fevereiro/2022] Introdu\u00e7\u00e3o \u00e0 Ci\u00eancia de Dados 3.0 - 12 Horas - Data Science Academy [Janeiro/2022] Fundamentos da Ci\u00eancia de Dados - 3 Horas - LinkedIn Learning [Janeiro/2022] Outros Forma\u00e7\u00e3o Come\u00e7ando em Cloud Computing - 6 Cursos - 53 Horas - Alura [Outubro/2023] Forma\u00e7\u00e3o Django: Crie Aplica\u00e7\u00f5es em Python - 4 Cursos - 38 Horas - Alura [Maio/2023] Python come\u00e7ando com a linguagem - 12 Horas - Alura [Abril/2023] Python na pr\u00e1tica (+150 exerc\u00edcios) - 3 Horas - Udemy - More Academy [Julho/2022] Power BI para Todos - 5 Horas - EIA - Fernando Amaral [Maio/2022] Machine Learning com Python - Regress\u00e3o Linear - 3 Horas - Souk Analytics - Data Viking [Abril/2022] Python 3 - Mundo 1 - 40 Horas - Curso em V\u00eddeo [Mar\u00e7o/2022] T\u00e9cnicas Avan\u00e7adas de Python - 3 Horas - LinkedIn Learning [Fevereiro/2022] Oracle Application Express - B\u00e1sico - 4 Horas - Udemy - Felipe Colacioppo [Mar\u00e7o/2021] Introdu\u00e7\u00e3o ao Sistema Operacional Linux - 2 Horas - Diego Mariano [Fevereiro/2022]","title":"Vida Acad\u00eamica"},{"location":"p3-academica/#vida-academica","text":"Como amante do estudo, a vida acad\u00eamica \u00e9 algo que sempre estar\u00e1 em atualiza\u00e7\u00e3o, pois como profissional de tecnologia sei que sempre estarei estudando, pois minha \u00e1rea est\u00e1 em constante evolu\u00e7\u00e3o e que assim eu possa evoluir junto. Est\u00e1 p\u00e1gina ira conter todas as minhas forma\u00e7\u00f5es acad\u00eamicas, como tamb\u00e9m todas as minhas certifica\u00e7\u00f5es e cursos livres, conte\u00fados esses que me constru\u00edram e me ajudaram a subir de n\u00edvel na minha carreira, como tamb\u00e9m a resolver diversos problemas dentro da minha organiza\u00e7\u00e3o.","title":"Vida Acad\u00eamica"},{"location":"p3-academica/#formacao-academica","text":"MBA em Business Intelligence na Faculdade Descomplica [Mar\u00e7o/2023] P\u00f3s-Graduado em An\u00e1lise de Dados na Faculdade Descomplica [Mar\u00e7o/2023] Tecn\u00f3logo em Gest\u00e3o da Tecnologia da Informa\u00e7\u00e3o na Faculdade Internacional da Para\u00edba-FPB [Junho/2014]","title":"Forma\u00e7\u00e3o Acad\u00eamica"},{"location":"p3-academica/#certificacoes","text":"Python (Basic) HackerRank [Novembro/2023] ISO 37001 (Antisuborno) e ISO 37301 (Complice) - Tradius [Outubro/2023] Business Intelligence Foundation Professional Certification BIFPC [Junho/2023]","title":"Certifica\u00e7\u00f5es"},{"location":"p3-academica/#cursos-livres","text":"","title":"Cursos Livres"},{"location":"p3-academica/#engenharia-de-dados","text":"Workshop 2 - Jornada de Dados 2024 - Pydantic, DataQuality e TDD para dados - 4 Horas - Luciano Galv\u00e3o [Fevereiro/2024] Workshop Data Science - Databricks, Spark e WebScraping - 4 Horas - Data Viking [Janeiro/2024] Databricks: Trabalhando com Diversos Formatos e Tipos de Arquivos - 8 Horas - Alura [Dezembro/2023] Databricks: Conhecendo a Ferramenta - 8 Horas - Alura [Novembro/2023] Spark: Apresentando a Ferramenta - 10 Horas - Alura [Setembro/2023] Forma\u00e7\u00e3o BI e Data Warehouse com SQL Server e Power BI - 6 Cursos - 60 Horas - Alura [Julho/2023] Transforma\u00e7\u00e3o com ETL: Pentaho Data Integration - 16 Horas - Alura [Maio/2023] Modelos de ETL: Pentaho Data Integration - 14 Horas - Alura [Abril/2023] Business Intelligence: Trabalhando com Data Warehouse - 12 Horas - Alura [Abril/2023] Python para Engenheiros e Cientistas / B\u00e1sico ao Avan\u00e7ado - 19 Horas - Udemy - Rafael Pereira [Maio/2022]","title":"Engenharia de Dados"},{"location":"p3-academica/#analise-de-dados","text":"Pandas: Conhecendo a Biblioteca - 8 Horas - Alura [Dezembro/2023] Estat\u00edstica com Python: Frequ\u00eancias e Medidas - 10 Horas - Alura [Novembro/2023] Data Visualization com Python: biblioteca Plotly - 5 Horas - Udemy - Data Viking [Outubro/2023] Data Science: Analise e Visualiza\u00e7\u00e3o de dados - 6 Horas - Alura [Maio/2023] Python para Data Science: Fun\u00e7\u00f5es, Pacotes e Pandas - 10 Horas - Alura [Maio/2023] Python para Data Science: Linguagem e Numpy - 12 Horas - Alura [Maio/2023] SQL para An\u00e1lise de Dados - 11 Horas - Udemy - Caio Avelino [Setembro/2022] Introdu\u00e7\u00e3o a Tableau - 5 Horas - EIA - Fernando Amaral [Abril/2022] Microsoft Power BI para Data Science 2.0 - 72 Horas - Data Science Academy [Abril/2022] Visualiza\u00e7\u00e3o de Dados (DataViz) - 10 Horas - Souk Analytics - Tiago Marques [Abril/2022] Python para Iniciantes: Aprenda do Zero - 5 Horas - EIA - Fernando Amaral [Abril/2022] Python para An\u00e1lise de Dados - 12 Horas - Udemy - Data Viking [Abril/2022] Dashboards Matadores: Domine a Arte de Apresenta\u00e7\u00e3o de Dados - 3 Horas - EIA - Fernando Amaral [Mar\u00e7o/2022] Data Science para Iniciantes: F\u00e1cil e Simples - 3 Horas - EIA - Fernando Amaral [Mar\u00e7o/2022] Python Fundamentos para An\u00e1lise de Dados 3.0 - 60 Horas - Data Science Academy [Mar\u00e7o/2022] Data Science: Visualiza\u00e7\u00e3o de dados com Python - 2 Horas - Diego Mariano [Fevereiro/2022] Power BI Completo - 9 Horas - Udemy - Jo\u00e3o Paulo [Fevereiro/2021]","title":"An\u00e1lise de Dados"},{"location":"p3-academica/#governanca","text":"Governan\u00e7a de TI: Gest\u00e3o de Requisitos no Contexto \u00c1gil - 8 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Gest\u00e3o de Requisitos no Contexto \u00c1gil - 8 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Gest\u00e3o de Programas, Projetos e Produtos - 8 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Gerencie Servi\u00e7os e Fortale\u00e7a a Seguran\u00e7a - 6 Horas - Alura [Fevereiro/2024] Governan\u00e7a de TI: Fundamentos da Gest\u00e3o de Portif\u00f3lios - 6 Horas - Alura [Janeiro/2024] Governan\u00e7a de TI: Modelo de Gest\u00e3o, Arquitetura e Inova\u00e7\u00e3o - 8 Horas - Alura [Janeiro/2024] Governan\u00e7a de TI: Alinhamento Estrat\u00e9gico - 8 Horas - Alura [Janeiro/2024] LGPD Registro das opera\u00e7\u00f5es de tratamento de Dados Pessoais - 3 Horas - Udemy - Cl\u00e1udio Dodt [Mar\u00e7o/2022] LGPD Zero: Conceitos fundamentais de Prote\u00e7\u00e3o Dados Pessoais - 1 Hora - Udemy - Cl\u00e1udio Dodt [Mar\u00e7o/2022] LGPD Pol\u00edtica de Privacidade e Prote\u00e7\u00e3o de Dados Pessoais - 4 Horas - Udemy - Cl\u00e1udio Dodt [Janeiro/2022] Fundamentos do Scrum - 1,5 Horas - Udemy - Pilar Sanchez [Janeiro/2021]","title":"Governan\u00e7a:"},{"location":"p3-academica/#dados","text":"Conceitos Gerais sobre Dados: O que voc\u00ea precisa Saber - 5 Horas - Udemy - Grimaldo Oliveira [Janeiro/2024] Modelagem de Banco de Dados Relacional: Normaliza\u00e7\u00e3o - 8 Horas - Alura [Novembro/2023] Modelagem de Banco de Dados Relacional: \u00c1lgebra Relacional - 8 Horas - Alura [Novembro/2023] Redes Neurais: Deep Learning com PyTorch - 6 Horas - Alura [Novembro/2023] PostgreSQL: Comandos DML e DDL - 8 Horas - Alura [Setembro/2023] PostgreSQL: Views, Sub-Consultas e Fun\u00e7\u00f5es - 6 Horas - Alura [Setembro/2023] PostgreSQL - 8 Horas - Alura [Julho/2023] Forma\u00e7\u00e3o Modelagem de Dados - 4 Cursos - 33 Horas - Alura [Julho/2023] Forma\u00e7\u00e3o Power BI - 5 Cursos - 55 Horas - Alura [Junho/2023] SQL com Python - 3 Horas - Souk Analytics - Data Viking [Abril/2022] Fundamentos do BigData T\u00e9cnicas e Conceitos - 2 Horas - LinkedIn Learning [Fevereiro/2022] Big Data Fundamentos 3.0 - 12 Horas - Data Science Academy [Fevereiro/2022] Python para Ci\u00eancia de Dados: Forma\u00e7\u00e3o B\u00e1sica - 4 Horas - LinkedIn Learning [Fevereiro/2022] Introdu\u00e7\u00e3o \u00e0 Ci\u00eancia de Dados 3.0 - 12 Horas - Data Science Academy [Janeiro/2022] Fundamentos da Ci\u00eancia de Dados - 3 Horas - LinkedIn Learning [Janeiro/2022]","title":"Dados"},{"location":"p3-academica/#outros","text":"Forma\u00e7\u00e3o Come\u00e7ando em Cloud Computing - 6 Cursos - 53 Horas - Alura [Outubro/2023] Forma\u00e7\u00e3o Django: Crie Aplica\u00e7\u00f5es em Python - 4 Cursos - 38 Horas - Alura [Maio/2023] Python come\u00e7ando com a linguagem - 12 Horas - Alura [Abril/2023] Python na pr\u00e1tica (+150 exerc\u00edcios) - 3 Horas - Udemy - More Academy [Julho/2022] Power BI para Todos - 5 Horas - EIA - Fernando Amaral [Maio/2022] Machine Learning com Python - Regress\u00e3o Linear - 3 Horas - Souk Analytics - Data Viking [Abril/2022] Python 3 - Mundo 1 - 40 Horas - Curso em V\u00eddeo [Mar\u00e7o/2022] T\u00e9cnicas Avan\u00e7adas de Python - 3 Horas - LinkedIn Learning [Fevereiro/2022] Oracle Application Express - B\u00e1sico - 4 Horas - Udemy - Felipe Colacioppo [Mar\u00e7o/2021] Introdu\u00e7\u00e3o ao Sistema Operacional Linux - 2 Horas - Diego Mariano [Fevereiro/2022]","title":"Outros"},{"location":"p4-experiencia/","text":"Experi\u00eancia Profissional Aqui estarei descrevendo algumas das minhas principais experi\u00eancias. Gerente de Projetos e Governan\u00e7a de Dados na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : 01/01/2024 [Emprego Atual] Atribui\u00e7\u00f5es : Minhas atribui\u00e7\u00f5es se iniciam no gerenciamento das seguintes equipes: equipe de dados, equipe de desenvolvimento, equipe de implanta\u00e7\u00e3o e equipe de suporte a sistemas. Al\u00e9m do gerenciamento das equipes, \u00e9 necess\u00e1rio acompanhar indicadores, metas/objetivos alcan\u00e7ados e realizar planejamentos estrat\u00e9gicos para a evolu\u00e7\u00e3o dos servi\u00e7os de TIC. Tamb\u00e9m sou respons\u00e1vel por toda a gest\u00e3o administrativa da Diretoria de Tecnologia em Sa\u00fade, da qual fa\u00e7o parte, direcionando a equipe de assessores administrativos. Como tamb\u00e9m desenvolver/aprimorar as politicas de dados da organiza\u00e7\u00e3o. Minhas tarefas n\u00e3o se resumem apenas \u00e0 gest\u00e3o, mas tamb\u00e9m incluem a\u00e7\u00f5es de opera\u00e7\u00e3o, como atuar em projetos de desenvolvimento web em Python Django, projetos de engenharia de dados e todas analises de requisitos. Em geral, utilizo amplamente ferramentas como BPMN, Kanban, Python, SQL, Django, Pentaho, Streamlit e MkDocs. T\u00e9cnico de N\u00edvel Superior - Analista de Dados na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : [01/04/2022] Sa\u00edda : [31/12/2023] Atribui\u00e7\u00f5es : As minhas principais atividades s\u00e3o monitorar os projetos de tecnologia em andamento, como tamb\u00e9m desenvolver novos projetos, al\u00e9m de Gerenciar todas estrutura de Dados da Secretaria Municipal de Sa\u00fade, Projetar/Executar/Monitorar sistemas de gerenciamento de dados que incluem: Banco de Dados Transacionais, Data Warehouse e Data Mart's, Pipeline de dados, garantindo assim atender as necessidades de neg\u00f3cios e a efici\u00eancia deles. As principais stacks utilizadas s\u00e3o: SQL, Python, Pentaho, Django. Principais Realiza\u00e7\u00f5es: Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web para Gest\u00e3o de Dados Gerais Implanta\u00e7\u00e3o de Prontu\u00e1rio Eletr\u00f4nico do Cidad\u00e3o em todas UPAS, Hospitais, Policl\u00ednicas e USF's do Municio de Jo\u00e3o Pessoa; Cria\u00e7\u00e3o de DataWarehouse voltado para Gestores de Sa\u00fade; Implanta\u00e7\u00e3o de Sistema de Auditoria Multidisciplinar em Hospitais e UPAS, realizando um mapeamento de dados que gerou economia mensal acima de R$ 1 milh\u00e3o; Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web de Interna\u00e7\u00e3o Hospitalar com Gest\u00e3o de Dados; Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web de Acompanhamento Oncol\u00f3gico com Gest\u00e3o de Dados; Lideran\u00e7a de Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web de Contratos de Servidores T\u00e9cnico de Suporte a Sistemas na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : [01/01/2021] Sa\u00edda : [31/03/2022] Atribui\u00e7\u00f5es : Suporte a Sistemas de Controle de Estoque, Implanta\u00e7\u00e3o de Sistema de Gest\u00e3o de Documentos (1Doc), Suporte a Sistema de Gest\u00e3o de Documentos (1Doc), Ministra\u00e7\u00e3o de Treinamentos, Prepara\u00e7\u00e3o de Treinamentos, Extra\u00e7\u00e3o de Dados de Sistemas, Elabora\u00e7\u00e3o de Dashboards para Tomada de Decis\u00f5es, Elabora\u00e7\u00e3o de Termos de Refer\u00eancias, Desenvolvimento de Sistemas Interno em LowCode(Oracle Apex), Utiliza\u00e7\u00e3o de Python para an\u00e1lises de Dados, Utiliza\u00e7\u00e3o do Power BI para visualiza\u00e7\u00e3o de Dados, Auxiliar a Dire\u00e7\u00e3o a tomada de decis\u00f5es e planejamentos estrat\u00e9gicos do setor. T\u00e9cnico em Administra\u00e7\u00e3o na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : [04/04/2018] Sa\u00edda : [31/12/2020] Atribui\u00e7\u00f5es : Atuando nas fun\u00e7\u00f5es administrativas relacionadas a inform\u00e1tica, como constru\u00e7\u00e3o de Estudo T\u00e9cnicos, elabora\u00e7\u00e3o de Termos de Refer\u00eancias para Projetos de TIC, monitoramento de chamados internos, elabora\u00e7\u00e3o de documentos internos, planejamento de aquisi\u00e7\u00f5es/contrata\u00e7\u00f5es de ativos de TIC.","title":"Experi\u00eancia Profissional"},{"location":"p4-experiencia/#experiencia-profissional","text":"Aqui estarei descrevendo algumas das minhas principais experi\u00eancias. Gerente de Projetos e Governan\u00e7a de Dados na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : 01/01/2024 [Emprego Atual] Atribui\u00e7\u00f5es : Minhas atribui\u00e7\u00f5es se iniciam no gerenciamento das seguintes equipes: equipe de dados, equipe de desenvolvimento, equipe de implanta\u00e7\u00e3o e equipe de suporte a sistemas. Al\u00e9m do gerenciamento das equipes, \u00e9 necess\u00e1rio acompanhar indicadores, metas/objetivos alcan\u00e7ados e realizar planejamentos estrat\u00e9gicos para a evolu\u00e7\u00e3o dos servi\u00e7os de TIC. Tamb\u00e9m sou respons\u00e1vel por toda a gest\u00e3o administrativa da Diretoria de Tecnologia em Sa\u00fade, da qual fa\u00e7o parte, direcionando a equipe de assessores administrativos. Como tamb\u00e9m desenvolver/aprimorar as politicas de dados da organiza\u00e7\u00e3o. Minhas tarefas n\u00e3o se resumem apenas \u00e0 gest\u00e3o, mas tamb\u00e9m incluem a\u00e7\u00f5es de opera\u00e7\u00e3o, como atuar em projetos de desenvolvimento web em Python Django, projetos de engenharia de dados e todas analises de requisitos. Em geral, utilizo amplamente ferramentas como BPMN, Kanban, Python, SQL, Django, Pentaho, Streamlit e MkDocs. T\u00e9cnico de N\u00edvel Superior - Analista de Dados na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : [01/04/2022] Sa\u00edda : [31/12/2023] Atribui\u00e7\u00f5es : As minhas principais atividades s\u00e3o monitorar os projetos de tecnologia em andamento, como tamb\u00e9m desenvolver novos projetos, al\u00e9m de Gerenciar todas estrutura de Dados da Secretaria Municipal de Sa\u00fade, Projetar/Executar/Monitorar sistemas de gerenciamento de dados que incluem: Banco de Dados Transacionais, Data Warehouse e Data Mart's, Pipeline de dados, garantindo assim atender as necessidades de neg\u00f3cios e a efici\u00eancia deles. As principais stacks utilizadas s\u00e3o: SQL, Python, Pentaho, Django. Principais Realiza\u00e7\u00f5es: Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web para Gest\u00e3o de Dados Gerais Implanta\u00e7\u00e3o de Prontu\u00e1rio Eletr\u00f4nico do Cidad\u00e3o em todas UPAS, Hospitais, Policl\u00ednicas e USF's do Municio de Jo\u00e3o Pessoa; Cria\u00e7\u00e3o de DataWarehouse voltado para Gestores de Sa\u00fade; Implanta\u00e7\u00e3o de Sistema de Auditoria Multidisciplinar em Hospitais e UPAS, realizando um mapeamento de dados que gerou economia mensal acima de R$ 1 milh\u00e3o; Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web de Interna\u00e7\u00e3o Hospitalar com Gest\u00e3o de Dados; Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web de Acompanhamento Oncol\u00f3gico com Gest\u00e3o de Dados; Lideran\u00e7a de Desenvolvimento/Implanta\u00e7\u00e3o de Sistema Web de Contratos de Servidores T\u00e9cnico de Suporte a Sistemas na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : [01/01/2021] Sa\u00edda : [31/03/2022] Atribui\u00e7\u00f5es : Suporte a Sistemas de Controle de Estoque, Implanta\u00e7\u00e3o de Sistema de Gest\u00e3o de Documentos (1Doc), Suporte a Sistema de Gest\u00e3o de Documentos (1Doc), Ministra\u00e7\u00e3o de Treinamentos, Prepara\u00e7\u00e3o de Treinamentos, Extra\u00e7\u00e3o de Dados de Sistemas, Elabora\u00e7\u00e3o de Dashboards para Tomada de Decis\u00f5es, Elabora\u00e7\u00e3o de Termos de Refer\u00eancias, Desenvolvimento de Sistemas Interno em LowCode(Oracle Apex), Utiliza\u00e7\u00e3o de Python para an\u00e1lises de Dados, Utiliza\u00e7\u00e3o do Power BI para visualiza\u00e7\u00e3o de Dados, Auxiliar a Dire\u00e7\u00e3o a tomada de decis\u00f5es e planejamentos estrat\u00e9gicos do setor. T\u00e9cnico em Administra\u00e7\u00e3o na Secretaria Municipal de Sa\u00fade de Jo\u00e3o Pessoa da Prefeitura Municipal de Jo\u00e3o Pessoa In\u00edcio : [04/04/2018] Sa\u00edda : [31/12/2020] Atribui\u00e7\u00f5es : Atuando nas fun\u00e7\u00f5es administrativas relacionadas a inform\u00e1tica, como constru\u00e7\u00e3o de Estudo T\u00e9cnicos, elabora\u00e7\u00e3o de Termos de Refer\u00eancias para Projetos de TIC, monitoramento de chamados internos, elabora\u00e7\u00e3o de documentos internos, planejamento de aquisi\u00e7\u00f5es/contrata\u00e7\u00f5es de ativos de TIC.","title":"Experi\u00eancia Profissional"},{"location":"p5-habilidades/","text":"Habilidades Hard Skills: Bancos de Dados Relacionais : SQL Server (T-SQL), Oracle (PL-SQL), MySQL, PostgreSQL Modelagem : Relacional, Dimensional Ferramentas de ETL : Pentaho, Integration Services, Pandas, PySpark Ferramentas de Dataviz : Power BI, Python Plotly, Python Matplotlib, Python Seaborn Ferramentas de apoio : Excel, GitHub, Kanban, BPMN, Trello, Azure DevOps Ferramentas de Documenta\u00e7\u00e3o : MKDocs Ferramentas de Desenvolvimento Web : Streamlit e Django Soft Skills: Habilidades de Comunica\u00e7\u00e3o Focado em Resolu\u00e7\u00e3o de Problemas Colabora\u00e7\u00e3o Pensamento Cr\u00edtico Constante Aprendizado Lideran\u00e7a","title":"Habilidades"},{"location":"p5-habilidades/#habilidades","text":"","title":"Habilidades"},{"location":"p5-habilidades/#hard-skills","text":"Bancos de Dados Relacionais : SQL Server (T-SQL), Oracle (PL-SQL), MySQL, PostgreSQL Modelagem : Relacional, Dimensional Ferramentas de ETL : Pentaho, Integration Services, Pandas, PySpark Ferramentas de Dataviz : Power BI, Python Plotly, Python Matplotlib, Python Seaborn Ferramentas de apoio : Excel, GitHub, Kanban, BPMN, Trello, Azure DevOps Ferramentas de Documenta\u00e7\u00e3o : MKDocs Ferramentas de Desenvolvimento Web : Streamlit e Django","title":"Hard Skills:"},{"location":"p5-habilidades/#soft-skills","text":"Habilidades de Comunica\u00e7\u00e3o Focado em Resolu\u00e7\u00e3o de Problemas Colabora\u00e7\u00e3o Pensamento Cr\u00edtico Constante Aprendizado Lideran\u00e7a","title":"Soft Skills:"},{"location":"p6-youtube/","text":"Canal Youtube Link do Canal Eu tamb\u00e9m estou produzindo conte\u00fado no Youtube, buscando falar sobre Projetos de Dados, tecnologias envolvidas no mundo de dados, como tamb\u00e9m um pouco sobre desenvolvimento, para que voc\u00ea possa me conhecer melhor vou contar uma pequena historia. Eu fui forjado atrav\u00e9s do conte\u00fado gratuito, que me formou, me ensinou, me direcionou, atrav\u00e9s de mentores no Youtube, de alguns mestres que compartilhavam CUPOM de gratuidade em cursos que eles gravaram, os cursos gratuitos da Udemy, os 2 meses free do LinkedIn, usei todo no LinkedIn Learning aprendendo, enfim conte\u00fado gratuito as vezes pode direcionar a todos n\u00f3s. Da aquela forcinha, se inscreva, curta e compartilhe. Espero que esse conte\u00fado ajude de alguma forma.","title":"Canal Youtube"},{"location":"p6-youtube/#canal-youtube","text":"Link do Canal Eu tamb\u00e9m estou produzindo conte\u00fado no Youtube, buscando falar sobre Projetos de Dados, tecnologias envolvidas no mundo de dados, como tamb\u00e9m um pouco sobre desenvolvimento, para que voc\u00ea possa me conhecer melhor vou contar uma pequena historia. Eu fui forjado atrav\u00e9s do conte\u00fado gratuito, que me formou, me ensinou, me direcionou, atrav\u00e9s de mentores no Youtube, de alguns mestres que compartilhavam CUPOM de gratuidade em cursos que eles gravaram, os cursos gratuitos da Udemy, os 2 meses free do LinkedIn, usei todo no LinkedIn Learning aprendendo, enfim conte\u00fado gratuito as vezes pode direcionar a todos n\u00f3s. Da aquela forcinha, se inscreva, curta e compartilhe. Espero que esse conte\u00fado ajude de alguma forma.","title":"Canal Youtube"},{"location":"pn-contato/","text":"Contato Existem v\u00e1rias formas de me contactar: Atrav\u00e9s do e-mail: luhborbafilho@gmail.com Atrav\u00e9s do Linkedin: Luciano Borba Atrav\u00e9s do Youtube: Canal Luciano Borba No GitHub: GitHub Luciano Borba Se conecte comigo:","title":"Contato"},{"location":"pn-contato/#contato","text":"Existem v\u00e1rias formas de me contactar: Atrav\u00e9s do e-mail: luhborbafilho@gmail.com Atrav\u00e9s do Linkedin: Luciano Borba Atrav\u00e9s do Youtube: Canal Luciano Borba No GitHub: GitHub Luciano Borba","title":"Contato"},{"location":"Projetos/p-7.1-DataGlowUP25/","text":"5DataGlowUP n\u00ba 25 GitHub do Projeto: DataGlowUp5-LuhBorba V\u00eddeo Explicativo Apresentando o Projeto - Youtube Stack do Projeto Python Pandas Matplotlib Numpy Jupyter Notebook Power Point Proposta do Projeto O projeto tem como objetivo fazer uma an\u00e1lise explorat\u00f3ria dos dados de partidas de League of Legends, dados estes dispon\u00edveis no Kaggle buscando focar n\u00e3o apenas nas tecnologias e fun\u00e7\u00f5es utilizadas, mas tamb\u00e9m na apresenta\u00e7\u00e3o simples e linguagem compat\u00edvel com gestores, para f\u00e1cil entendimento. Sum\u00e1rio do C\u00f3digo Explorando Dados Transforma\u00e7\u00e3o de Dados Analisando Dados 1- Explorando Dados Antes de explorar os dados, utilizei uma c\u00e9lula exclusiva para importa\u00e7\u00e3o de bibliotecas. # Importando Bibliotecas import pandas as pd import matplotlib.pyplot as plt import numpy as np Ap\u00f3s importa\u00e7\u00e3o das bibliotecas, fiz o carregamento dos dados, como tamb\u00e9m alguns op\u00e7\u00f5es para obter melhores informa\u00e7\u00f5es: # Carregando dados df = pd.read_csv('lol_ranked_games.csv') # Verificando 5 primeiras linhas df.head() # Verificando Schema df.info() # Verificando contagem de valores do gameId contagem_gameId = df['gameId'].value_counts() print(contagem_gameId) 2- Transformando Dados Nesta etapa, primeiramente criei um campo de KDA, verificando a quantidade de assassinatos (Kills), assist\u00eancias e mortes por jogo, considerando que caso a quantidade de mortes seja 0 n\u00e3o de nenhum error. # Criando campo de KDA, Verificando se o n\u00famero de mortes \u00e9 igual a 0 para n\u00e3o criar valores com erro df['kda'] = np.where(df['deaths'] == 0, (df['kills'] + df['assists']), (df['kills'] + df['assists']) /( df['deaths'] )) Ap\u00f3s cria\u00e7\u00e3o, ajustei as casas decimais do novo campo criado df['kda'] = df['kda'].round(2) Posteriormente criei um novo DataFrame utilizando apenas as colunas que irei utilizar para retirar insights. # Criando novo DF com dados que ser\u00e3o apenas utilizados df_final = df[[ 'gameId', 'hasWon', 'goldDiff', 'champLevelDiff', 'isFirstTower', 'isFirstBlood', 'killedElderDrake', 'killedBaronNashor', 'kills', 'deaths', 'assists', 'wardsPlaced', 'wardsDestroyed', 'wardsLost', 'kda' ]] 3- Realizando An\u00e1lises 3.1 - An\u00e1lises comparativas Primeiramente criei um DataFrame para vit\u00f3rias e outro para derrotas. Para fazer compara\u00e7\u00f5es de alguns dados. # Filtrando o DataFrame para quando o time venceu (hasWon == 1) vitorias = df_final[df_final['hasWon'] == 1] # Filtrando o DataFrame para quando o time perdeu (hasWon == 0) derrotas = df_final[df_final['hasWon'] == 0] Calculando comparativo de KDA: media_kda_vitorias = vitorias['kda'].mean() media_kda_derrotas = derrotas['kda'].mean() # Exibindo os resultados print(\"M\u00e9dia do KDA para vit\u00f3rias:\", media_kda_vitorias.round(2)) print(\"M\u00e9dia do KDA para derrotas:\", media_kda_derrotas.round(2)) Calculando comparativo de Wards Colocadas: # Calculando a m\u00e9dia de wardsPlaced para vit\u00f3rias e derrotas media_wardsPlaced_vitorias = vitorias['wardsPlaced'].mean() media_wardsPlaced_derrotas = derrotas['wardsPlaced'].mean() # Exibindo os resultados print(\"M\u00e9dia de wardsPlaced para vit\u00f3rias:\", media_wardsPlaced_vitorias.round(2)) print(\"M\u00e9dia de wardsPlaced para derrotas:\", media_wardsPlaced_derrotas.round(2)) Calculando comparativo de Wards Destru\u00eddas: # Calculando a m\u00e9dia de wardsDestroyed para vit\u00f3rias e derrotas media_wardsDestroyed_vitorias = vitorias['wardsDestroyed'].mean() media_wardsDestroyed_derrotas = derrotas['wardsDestroyed'].mean() # Exibindo os resultados print(\"M\u00e9dia de wardsDestroyed para vit\u00f3rias:\", media_wardsDestroyed_vitorias.round(2)) print(\"M\u00e9dia de wardsDestroyed para derrotas:\", media_wardsDestroyed_derrotas.round(2)) Calculando comparativo de Diferen\u00e7a de Gold: # Calculando a m\u00e9dia de goldDiff para vit\u00f3rias e derrotas media_goldDiff_vitorias = vitorias['goldDiff'].mean() media_goldDiff_derrotas = derrotas['goldDiff'].mean() # Exibindo os resultados print(\"M\u00e9dia de goldDiff para vit\u00f3rias:\", media_goldDiff_vitorias.round(2)) print(\"M\u00e9dia de goldDiff para derrotas:\", media_goldDiff_derrotas.round(2)) 3.2 - An\u00e1lises Gerais Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo KDA: Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de kda, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do KDA m\u00e9dio. # Calculando o valor m\u00e9dio do KDA valor_medio_kda = df_final['kda'].mean() # Filtrando as partidas que o KDA est\u00e1 acima da m\u00e9dia partidas_kda_maior_media = df_final[df_final['kda'] > valor_medio_kda] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que o KDA \u00e9 acima da m\u00e9dia vitorias_kda_maior_media = partidas_kda_maior_media[partidas_kda_maior_media['hasWon'] == 1] derrotas_kda_maior_media = partidas_kda_maior_media[partidas_kda_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_kda_maior_media = len(vitorias_kda_maior_media) / len(partidas_kda_maior_media) # Exibindo os resultados print(\"Valor m\u00e9dio do KDA:\", valor_medio_kda.round(2)) print('-----------------') print(\"N\u00famero de partidas com KDA acima da m\u00e9dia:\", len(partidas_kda_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com KDA acima da m\u00e9dia:\", len(vitorias_kda_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com KDA acima da m\u00e9dia:\", len(derrotas_kda_maior_media)) print('-----------------') print(\"Chance de vit\u00f3ria com KDA acima da m\u00e9dia: {:.2f}\".format(chance_vitoria_kda_maior_media)) Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo WardsPlaced (Vis\u00f5es Colocadas) : Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de WardsPlaced, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do WardsPlaced m\u00e9dio. # Calculando o valor m\u00e9dio de wardsPlaced valor_medio_wardsPlaced = df_final['wardsPlaced'].mean() # Filtrando as partidas em que wardsPlaced \u00e9 acima da m\u00e9dia partidas_wardsPlaced_maior_media = df[df['wardsPlaced'] > valor_medio_wardsPlaced] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que wardsPlaced \u00e9 acima da m\u00e9dia vitorias_wardsPlaced_maior_media = partidas_wardsPlaced_maior_media[partidas_wardsPlaced_maior_media['hasWon'] == 1] derrotas_wardsPlaced_maior_media = partidas_wardsPlaced_maior_media[partidas_wardsPlaced_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_wardsPlaced_maior_media = len(vitorias_wardsPlaced_maior_media) / len(partidas_wardsPlaced_maior_media) # Exibindo valores print(\"Valor m\u00e9dio de wardsPlaced:\", valor_medio_wardsPlaced.round(2)) print('-----------------') print(\"N\u00famero de partidas com wardsPlaced maior que a m\u00e9dia:\", len(partidas_wardsPlaced_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com wardsPlaced maior que a m\u00e9dia:\", len(vitorias_wardsPlaced_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com wardsPlaced maior que a m\u00e9dia:\", len(derrotas_wardsPlaced_maior_media)) print('-----------------') print(f\"Chance de vit\u00f3ria com wardsPlaced maior que a m\u00e9dia: {chance_vitoria_wardsPlaced_maior_media:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo GoldDiff (Diferen\u00e7a de Ouro) : Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de GoldDiff, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do GoldDiff m\u00e9dio. # Calculando o valor m\u00e9dio de goldDiff valor_medio_goldDiff = df_final['goldDiff'].mean() # Filtrando as partidas em que goldDiff \u00e9 acima da m\u00e9dia partidas_goldDiff_maior_media = df[df['goldDiff'] > valor_medio_goldDiff] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que goldDiff \u00e9 acima da m\u00e9dia vitorias_goldDiff_maior_media = partidas_goldDiff_maior_media[partidas_goldDiff_maior_media['hasWon'] == 1] derrotas_goldDiff_maior_media = partidas_goldDiff_maior_media[partidas_goldDiff_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_goldDiff_maior_media = len(vitorias_goldDiff_maior_media) / len(partidas_goldDiff_maior_media) # Exibindo Valores print(\"Valor m\u00e9dio de goldDiff:\", valor_medio_goldDiff.round(2)) print('-----------------') print(\"N\u00famero de partidas com goldDiff maior que a m\u00e9dia:\", len(partidas_goldDiff_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com goldDiff maior que a m\u00e9dia:\", len(vitorias_goldDiff_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com goldDiff maior que a m\u00e9dia:\", len(derrotas_goldDiff_maior_media)) print('-----------------') print(f\"Chance de vit\u00f3ria com goldDiff maior que a m\u00e9dia: {chance_vitoria_goldDiff_maior_media:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo ChampLevelDiff (Diferen\u00e7a de Level) : Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de ChampLevelDiff, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do ChampLevelDiff m\u00e9dio. # Calculando o valor m\u00e9dio de champLevelDiff valor_medio_champLevelDiff = df_final['champLevelDiff'].mean() # Filtrando as partidas em que champLevelDiff \u00e9 acima da m\u00e9dia partidas_champLevelDiff_maior_media = df_final[df_final['champLevelDiff'] > valor_medio_champLevelDiff] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que champLevelDiff \u00e9 acima da m\u00e9dia vitorias_champLevelDiff_maior_media = partidas_champLevelDiff_maior_media[partidas_champLevelDiff_maior_media['hasWon'] == 1] derrotas_champLevelDiff_maior_media = partidas_champLevelDiff_maior_media[partidas_champLevelDiff_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_champLevelDiff_maior_media = len(vitorias_champLevelDiff_maior_media) / len(partidas_champLevelDiff_maior_media) # Exibindo Valores print(\"Valor m\u00e9dio de champLevelDiff:\", valor_medio_champLevelDiff.round(2)) print('-----------------') print(\"N\u00famero de partidas com champLevelDiff maior que a m\u00e9dia:\", len(partidas_champLevelDiff_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com champLevelDiff maior que a m\u00e9dia:\", len(vitorias_champLevelDiff_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com champLevelDiff maior que a m\u00e9dia:\", len(derrotas_champLevelDiff_maior_media)) print('-----------------') print(f\"Chance de vit\u00f3ria com champLevelDiff maior que a m\u00e9dia: {chance_vitoria_champLevelDiff_maior_media:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo isFirstTower (Primeira Torre) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve isFirstTower, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrando as partidas em que isFirstTower foi derrubada partidas_firstTower = df[df['isFirstTower'] == 1] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que isFirstTower foi derrubada vitorias_firstTower = partidas_firstTower[partidas_firstTower['hasWon'] == 1] derrotas_firstTower = partidas_firstTower[partidas_firstTower['hasWon'] == 0] # Calculando a chance de vit\u00f3ria quando isFirstTower foi derrubada chance_vitoria_firstTower = len(vitorias_firstTower) / len(partidas_firstTower) print(\"N\u00famero de partidas em que isFirstTower \u00e9 igual a 1:\", len(partidas_firstTower)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando isFirstTower \u00e9 igual a 1:\", len(vitorias_firstTower)) print('-----------------') print(\"N\u00famero de derrotas quando isFirstTower \u00e9 igual a 1:\", len(derrotas_firstTower)) print('-----------------') print(f\"Chance de vit\u00f3ria quando isFirstTower \u00e9 igual a 1: {chance_vitoria_firstTower:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo isFirstBlood (Primeira Assassinato) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve isFirstBlood, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrando as partidas em que isFirstBlood foi realizado partidas_firstBlood = df[df['isFirstBlood'] == 1] # Contar o n\u00famero de vit\u00f3rias e derrotas nas partidas em que isFirstBlood foi realizado vitorias_firstBlood = partidas_firstBlood[partidas_firstBlood['hasWon'] == 1] derrotas_firstBlood = partidas_firstBlood[partidas_firstBlood['hasWon'] == 0] # Calcular a chance de vit\u00f3ria quando isFirstBlood foi realizado chance_vitoria_firstBlood = len(vitorias_firstBlood) / len(partidas_firstBlood) # Exibindo Resultados print(\"N\u00famero de partidas em que isFirstBlood \u00e9 igual a 1:\", len(partidas_firstBlood)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando isFirstBlood \u00e9 igual a 1:\", len(vitorias_firstBlood)) print('-----------------') print(\"N\u00famero de derrotas quando isFirstBlood \u00e9 igual a 1:\", len(derrotas_firstBlood)) print('-----------------') print(f\"Chance de vit\u00f3ria quando isFirstBlood \u00e9 igual a 1: {chance_vitoria_firstBlood:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo killedElderDrake (Matou Drag\u00e3o Anci\u00e3o) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve killedElderDrake, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrando as partidas em que Drag\u00e3o Anci\u00e3o foi Derrotado partidas_elder_true = df_final[df_final['killedElderDrake'] == 1] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que Drag\u00e3o Anci\u00e3o foi Derrotado vitorias_elder_true = partidas_elder_true[partidas_elder_true['hasWon'] == 1] derrotas_elder_true = partidas_elder_true[partidas_elder_true['hasWon'] == 0] # Calculando a chance de vit\u00f3ria quando Drag\u00e3o Anci\u00e3o foi Derrotado chance_vitoria_elder_true = len(vitorias_elder_true) / len(partidas_elder_true) # Exibindo Resultados print(\"N\u00famero de partidas em que o Drag\u00e3o Anci\u00e3o foi Derrotado:\", len(partidas_elder_true)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando o Drag\u00e3o Anci\u00e3o foi Derrotado:\", len(vitorias_elder_true)) print('-----------------') print(\"N\u00famero de derrotas quando o Drag\u00e3o Anci\u00e3o foi Derrotado:\", len(derrotas_elder_true)) print('-----------------') print(f\"Chance de vit\u00f3ria quando o Drag\u00e3o Anci\u00e3o foi Derrotado: {chance_vitoria_elder_true:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo killedBaronNashor (Matou Baron Nashor) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve killedBaronNashor, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrar as partidas em que Bar\u00e3o Nashor foi Derrotado partidas_baron_true = df_final[df_final['killedBaronNashor'] == 1] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que Bar\u00e3o Nashor foi Derrotado vitorias_baron_true = partidas_baron_true[partidas_baron_true['hasWon'] == 1] derrotas_baron_true = partidas_baron_true[partidas_baron_true['hasWon'] == 0] # Calculando a chance de vit\u00f3ria quando o Bar\u00e3o Nashor foi Derrotado chance_vitoria_baron_true = len(vitorias_baron_true) / len(partidas_baron_true) # Exibindo resultados print(\"N\u00famero de partidas em que killedBaronNashor \u00e9 True:\", len(partidas_baron_true)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando killedBaronNashor \u00e9 True:\", len(vitorias_baron_true)) print('-----------------') print(\"N\u00famero de derrotas quando killedBaronNashor \u00e9 True:\", len(derrotas_baron_true)) print('-----------------') print(f\"Chance de vit\u00f3ria quando killedBaronNashor \u00e9 True: {chance_vitoria_baron_true:.2f}\") Conclus\u00e3o Se voc\u00ea deseja avaliar melhor o c\u00f3digo clique aqui , se desejar assistir o v\u00eddeo de explica\u00e7\u00e3o dos resultados clique aqui . Ao ir para o v\u00eddeo me da uma moralzinha, curte, comenta o que voc\u00ea achou, se inscreve no canal e compartilha. Obrigado","title":"5DataGlowUP n\u00ba 25"},{"location":"Projetos/p-7.1-DataGlowUP25/#5dataglowup-no-25","text":"GitHub do Projeto: DataGlowUp5-LuhBorba V\u00eddeo Explicativo Apresentando o Projeto - Youtube","title":"5DataGlowUP n\u00ba 25"},{"location":"Projetos/p-7.1-DataGlowUP25/#stack-do-projeto","text":"Python Pandas Matplotlib Numpy Jupyter Notebook Power Point","title":"Stack do Projeto"},{"location":"Projetos/p-7.1-DataGlowUP25/#proposta-do-projeto","text":"O projeto tem como objetivo fazer uma an\u00e1lise explorat\u00f3ria dos dados de partidas de League of Legends, dados estes dispon\u00edveis no Kaggle buscando focar n\u00e3o apenas nas tecnologias e fun\u00e7\u00f5es utilizadas, mas tamb\u00e9m na apresenta\u00e7\u00e3o simples e linguagem compat\u00edvel com gestores, para f\u00e1cil entendimento.","title":"Proposta do Projeto"},{"location":"Projetos/p-7.1-DataGlowUP25/#sumario-do-codigo","text":"Explorando Dados Transforma\u00e7\u00e3o de Dados Analisando Dados","title":"Sum\u00e1rio do C\u00f3digo"},{"location":"Projetos/p-7.1-DataGlowUP25/#1-explorando-dados","text":"Antes de explorar os dados, utilizei uma c\u00e9lula exclusiva para importa\u00e7\u00e3o de bibliotecas. # Importando Bibliotecas import pandas as pd import matplotlib.pyplot as plt import numpy as np Ap\u00f3s importa\u00e7\u00e3o das bibliotecas, fiz o carregamento dos dados, como tamb\u00e9m alguns op\u00e7\u00f5es para obter melhores informa\u00e7\u00f5es: # Carregando dados df = pd.read_csv('lol_ranked_games.csv') # Verificando 5 primeiras linhas df.head() # Verificando Schema df.info() # Verificando contagem de valores do gameId contagem_gameId = df['gameId'].value_counts() print(contagem_gameId)","title":"1- Explorando Dados"},{"location":"Projetos/p-7.1-DataGlowUP25/#2-transformando-dados","text":"Nesta etapa, primeiramente criei um campo de KDA, verificando a quantidade de assassinatos (Kills), assist\u00eancias e mortes por jogo, considerando que caso a quantidade de mortes seja 0 n\u00e3o de nenhum error. # Criando campo de KDA, Verificando se o n\u00famero de mortes \u00e9 igual a 0 para n\u00e3o criar valores com erro df['kda'] = np.where(df['deaths'] == 0, (df['kills'] + df['assists']), (df['kills'] + df['assists']) /( df['deaths'] )) Ap\u00f3s cria\u00e7\u00e3o, ajustei as casas decimais do novo campo criado df['kda'] = df['kda'].round(2) Posteriormente criei um novo DataFrame utilizando apenas as colunas que irei utilizar para retirar insights. # Criando novo DF com dados que ser\u00e3o apenas utilizados df_final = df[[ 'gameId', 'hasWon', 'goldDiff', 'champLevelDiff', 'isFirstTower', 'isFirstBlood', 'killedElderDrake', 'killedBaronNashor', 'kills', 'deaths', 'assists', 'wardsPlaced', 'wardsDestroyed', 'wardsLost', 'kda' ]]","title":"2- Transformando Dados"},{"location":"Projetos/p-7.1-DataGlowUP25/#3-realizando-analises","text":"","title":"3- Realizando An\u00e1lises"},{"location":"Projetos/p-7.1-DataGlowUP25/#31-analises-comparativas","text":"Primeiramente criei um DataFrame para vit\u00f3rias e outro para derrotas. Para fazer compara\u00e7\u00f5es de alguns dados. # Filtrando o DataFrame para quando o time venceu (hasWon == 1) vitorias = df_final[df_final['hasWon'] == 1] # Filtrando o DataFrame para quando o time perdeu (hasWon == 0) derrotas = df_final[df_final['hasWon'] == 0] Calculando comparativo de KDA: media_kda_vitorias = vitorias['kda'].mean() media_kda_derrotas = derrotas['kda'].mean() # Exibindo os resultados print(\"M\u00e9dia do KDA para vit\u00f3rias:\", media_kda_vitorias.round(2)) print(\"M\u00e9dia do KDA para derrotas:\", media_kda_derrotas.round(2)) Calculando comparativo de Wards Colocadas: # Calculando a m\u00e9dia de wardsPlaced para vit\u00f3rias e derrotas media_wardsPlaced_vitorias = vitorias['wardsPlaced'].mean() media_wardsPlaced_derrotas = derrotas['wardsPlaced'].mean() # Exibindo os resultados print(\"M\u00e9dia de wardsPlaced para vit\u00f3rias:\", media_wardsPlaced_vitorias.round(2)) print(\"M\u00e9dia de wardsPlaced para derrotas:\", media_wardsPlaced_derrotas.round(2)) Calculando comparativo de Wards Destru\u00eddas: # Calculando a m\u00e9dia de wardsDestroyed para vit\u00f3rias e derrotas media_wardsDestroyed_vitorias = vitorias['wardsDestroyed'].mean() media_wardsDestroyed_derrotas = derrotas['wardsDestroyed'].mean() # Exibindo os resultados print(\"M\u00e9dia de wardsDestroyed para vit\u00f3rias:\", media_wardsDestroyed_vitorias.round(2)) print(\"M\u00e9dia de wardsDestroyed para derrotas:\", media_wardsDestroyed_derrotas.round(2)) Calculando comparativo de Diferen\u00e7a de Gold: # Calculando a m\u00e9dia de goldDiff para vit\u00f3rias e derrotas media_goldDiff_vitorias = vitorias['goldDiff'].mean() media_goldDiff_derrotas = derrotas['goldDiff'].mean() # Exibindo os resultados print(\"M\u00e9dia de goldDiff para vit\u00f3rias:\", media_goldDiff_vitorias.round(2)) print(\"M\u00e9dia de goldDiff para derrotas:\", media_goldDiff_derrotas.round(2))","title":"3.1 - An\u00e1lises comparativas"},{"location":"Projetos/p-7.1-DataGlowUP25/#32-analises-gerais","text":"Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo KDA: Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de kda, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do KDA m\u00e9dio. # Calculando o valor m\u00e9dio do KDA valor_medio_kda = df_final['kda'].mean() # Filtrando as partidas que o KDA est\u00e1 acima da m\u00e9dia partidas_kda_maior_media = df_final[df_final['kda'] > valor_medio_kda] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que o KDA \u00e9 acima da m\u00e9dia vitorias_kda_maior_media = partidas_kda_maior_media[partidas_kda_maior_media['hasWon'] == 1] derrotas_kda_maior_media = partidas_kda_maior_media[partidas_kda_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_kda_maior_media = len(vitorias_kda_maior_media) / len(partidas_kda_maior_media) # Exibindo os resultados print(\"Valor m\u00e9dio do KDA:\", valor_medio_kda.round(2)) print('-----------------') print(\"N\u00famero de partidas com KDA acima da m\u00e9dia:\", len(partidas_kda_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com KDA acima da m\u00e9dia:\", len(vitorias_kda_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com KDA acima da m\u00e9dia:\", len(derrotas_kda_maior_media)) print('-----------------') print(\"Chance de vit\u00f3ria com KDA acima da m\u00e9dia: {:.2f}\".format(chance_vitoria_kda_maior_media)) Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo WardsPlaced (Vis\u00f5es Colocadas) : Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de WardsPlaced, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do WardsPlaced m\u00e9dio. # Calculando o valor m\u00e9dio de wardsPlaced valor_medio_wardsPlaced = df_final['wardsPlaced'].mean() # Filtrando as partidas em que wardsPlaced \u00e9 acima da m\u00e9dia partidas_wardsPlaced_maior_media = df[df['wardsPlaced'] > valor_medio_wardsPlaced] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que wardsPlaced \u00e9 acima da m\u00e9dia vitorias_wardsPlaced_maior_media = partidas_wardsPlaced_maior_media[partidas_wardsPlaced_maior_media['hasWon'] == 1] derrotas_wardsPlaced_maior_media = partidas_wardsPlaced_maior_media[partidas_wardsPlaced_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_wardsPlaced_maior_media = len(vitorias_wardsPlaced_maior_media) / len(partidas_wardsPlaced_maior_media) # Exibindo valores print(\"Valor m\u00e9dio de wardsPlaced:\", valor_medio_wardsPlaced.round(2)) print('-----------------') print(\"N\u00famero de partidas com wardsPlaced maior que a m\u00e9dia:\", len(partidas_wardsPlaced_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com wardsPlaced maior que a m\u00e9dia:\", len(vitorias_wardsPlaced_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com wardsPlaced maior que a m\u00e9dia:\", len(derrotas_wardsPlaced_maior_media)) print('-----------------') print(f\"Chance de vit\u00f3ria com wardsPlaced maior que a m\u00e9dia: {chance_vitoria_wardsPlaced_maior_media:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo GoldDiff (Diferen\u00e7a de Ouro) : Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de GoldDiff, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do GoldDiff m\u00e9dio. # Calculando o valor m\u00e9dio de goldDiff valor_medio_goldDiff = df_final['goldDiff'].mean() # Filtrando as partidas em que goldDiff \u00e9 acima da m\u00e9dia partidas_goldDiff_maior_media = df[df['goldDiff'] > valor_medio_goldDiff] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que goldDiff \u00e9 acima da m\u00e9dia vitorias_goldDiff_maior_media = partidas_goldDiff_maior_media[partidas_goldDiff_maior_media['hasWon'] == 1] derrotas_goldDiff_maior_media = partidas_goldDiff_maior_media[partidas_goldDiff_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_goldDiff_maior_media = len(vitorias_goldDiff_maior_media) / len(partidas_goldDiff_maior_media) # Exibindo Valores print(\"Valor m\u00e9dio de goldDiff:\", valor_medio_goldDiff.round(2)) print('-----------------') print(\"N\u00famero de partidas com goldDiff maior que a m\u00e9dia:\", len(partidas_goldDiff_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com goldDiff maior que a m\u00e9dia:\", len(vitorias_goldDiff_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com goldDiff maior que a m\u00e9dia:\", len(derrotas_goldDiff_maior_media)) print('-----------------') print(f\"Chance de vit\u00f3ria com goldDiff maior que a m\u00e9dia: {chance_vitoria_goldDiff_maior_media:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo ChampLevelDiff (Diferen\u00e7a de Level) : Basicamente nesta etapa criei um vari\u00e1vel de valor m\u00e9dio de ChampLevelDiff, para assim calcular a probabilidade de vit\u00f3ria quando se esta acima do ChampLevelDiff m\u00e9dio. # Calculando o valor m\u00e9dio de champLevelDiff valor_medio_champLevelDiff = df_final['champLevelDiff'].mean() # Filtrando as partidas em que champLevelDiff \u00e9 acima da m\u00e9dia partidas_champLevelDiff_maior_media = df_final[df_final['champLevelDiff'] > valor_medio_champLevelDiff] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que champLevelDiff \u00e9 acima da m\u00e9dia vitorias_champLevelDiff_maior_media = partidas_champLevelDiff_maior_media[partidas_champLevelDiff_maior_media['hasWon'] == 1] derrotas_champLevelDiff_maior_media = partidas_champLevelDiff_maior_media[partidas_champLevelDiff_maior_media['hasWon'] == 0] # Calculando a chance de vit\u00f3ria chance_vitoria_champLevelDiff_maior_media = len(vitorias_champLevelDiff_maior_media) / len(partidas_champLevelDiff_maior_media) # Exibindo Valores print(\"Valor m\u00e9dio de champLevelDiff:\", valor_medio_champLevelDiff.round(2)) print('-----------------') print(\"N\u00famero de partidas com champLevelDiff maior que a m\u00e9dia:\", len(partidas_champLevelDiff_maior_media)) print('-----------------') print(\"N\u00famero de vit\u00f3rias com champLevelDiff maior que a m\u00e9dia:\", len(vitorias_champLevelDiff_maior_media)) print('-----------------') print(\"N\u00famero de derrotas com champLevelDiff maior que a m\u00e9dia:\", len(derrotas_champLevelDiff_maior_media)) print('-----------------') print(f\"Chance de vit\u00f3ria com champLevelDiff maior que a m\u00e9dia: {chance_vitoria_champLevelDiff_maior_media:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo isFirstTower (Primeira Torre) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve isFirstTower, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrando as partidas em que isFirstTower foi derrubada partidas_firstTower = df[df['isFirstTower'] == 1] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que isFirstTower foi derrubada vitorias_firstTower = partidas_firstTower[partidas_firstTower['hasWon'] == 1] derrotas_firstTower = partidas_firstTower[partidas_firstTower['hasWon'] == 0] # Calculando a chance de vit\u00f3ria quando isFirstTower foi derrubada chance_vitoria_firstTower = len(vitorias_firstTower) / len(partidas_firstTower) print(\"N\u00famero de partidas em que isFirstTower \u00e9 igual a 1:\", len(partidas_firstTower)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando isFirstTower \u00e9 igual a 1:\", len(vitorias_firstTower)) print('-----------------') print(\"N\u00famero de derrotas quando isFirstTower \u00e9 igual a 1:\", len(derrotas_firstTower)) print('-----------------') print(f\"Chance de vit\u00f3ria quando isFirstTower \u00e9 igual a 1: {chance_vitoria_firstTower:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo isFirstBlood (Primeira Assassinato) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve isFirstBlood, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrando as partidas em que isFirstBlood foi realizado partidas_firstBlood = df[df['isFirstBlood'] == 1] # Contar o n\u00famero de vit\u00f3rias e derrotas nas partidas em que isFirstBlood foi realizado vitorias_firstBlood = partidas_firstBlood[partidas_firstBlood['hasWon'] == 1] derrotas_firstBlood = partidas_firstBlood[partidas_firstBlood['hasWon'] == 0] # Calcular a chance de vit\u00f3ria quando isFirstBlood foi realizado chance_vitoria_firstBlood = len(vitorias_firstBlood) / len(partidas_firstBlood) # Exibindo Resultados print(\"N\u00famero de partidas em que isFirstBlood \u00e9 igual a 1:\", len(partidas_firstBlood)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando isFirstBlood \u00e9 igual a 1:\", len(vitorias_firstBlood)) print('-----------------') print(\"N\u00famero de derrotas quando isFirstBlood \u00e9 igual a 1:\", len(derrotas_firstBlood)) print('-----------------') print(f\"Chance de vit\u00f3ria quando isFirstBlood \u00e9 igual a 1: {chance_vitoria_firstBlood:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo killedElderDrake (Matou Drag\u00e3o Anci\u00e3o) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve killedElderDrake, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrando as partidas em que Drag\u00e3o Anci\u00e3o foi Derrotado partidas_elder_true = df_final[df_final['killedElderDrake'] == 1] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que Drag\u00e3o Anci\u00e3o foi Derrotado vitorias_elder_true = partidas_elder_true[partidas_elder_true['hasWon'] == 1] derrotas_elder_true = partidas_elder_true[partidas_elder_true['hasWon'] == 0] # Calculando a chance de vit\u00f3ria quando Drag\u00e3o Anci\u00e3o foi Derrotado chance_vitoria_elder_true = len(vitorias_elder_true) / len(partidas_elder_true) # Exibindo Resultados print(\"N\u00famero de partidas em que o Drag\u00e3o Anci\u00e3o foi Derrotado:\", len(partidas_elder_true)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando o Drag\u00e3o Anci\u00e3o foi Derrotado:\", len(vitorias_elder_true)) print('-----------------') print(\"N\u00famero de derrotas quando o Drag\u00e3o Anci\u00e3o foi Derrotado:\", len(derrotas_elder_true)) print('-----------------') print(f\"Chance de vit\u00f3ria quando o Drag\u00e3o Anci\u00e3o foi Derrotado: {chance_vitoria_elder_true:.2f}\") Calculando Chance de Vit\u00f3ria atrav\u00e9s do campo killedBaronNashor (Matou Baron Nashor) : Basicamente nesta etapa criei um vari\u00e1vel de verificando se o time vencedor obteve killedBaronNashor, para assim calcular a probabilidade de vit\u00f3ria esta condi\u00e7\u00e3o \u00e9 verdadeira. # Filtrar as partidas em que Bar\u00e3o Nashor foi Derrotado partidas_baron_true = df_final[df_final['killedBaronNashor'] == 1] # Contando o n\u00famero de vit\u00f3rias e derrotas nas partidas em que Bar\u00e3o Nashor foi Derrotado vitorias_baron_true = partidas_baron_true[partidas_baron_true['hasWon'] == 1] derrotas_baron_true = partidas_baron_true[partidas_baron_true['hasWon'] == 0] # Calculando a chance de vit\u00f3ria quando o Bar\u00e3o Nashor foi Derrotado chance_vitoria_baron_true = len(vitorias_baron_true) / len(partidas_baron_true) # Exibindo resultados print(\"N\u00famero de partidas em que killedBaronNashor \u00e9 True:\", len(partidas_baron_true)) print('-----------------') print(\"N\u00famero de vit\u00f3rias quando killedBaronNashor \u00e9 True:\", len(vitorias_baron_true)) print('-----------------') print(\"N\u00famero de derrotas quando killedBaronNashor \u00e9 True:\", len(derrotas_baron_true)) print('-----------------') print(f\"Chance de vit\u00f3ria quando killedBaronNashor \u00e9 True: {chance_vitoria_baron_true:.2f}\")","title":"3.2 - An\u00e1lises Gerais"},{"location":"Projetos/p-7.1-DataGlowUP25/#conclusao","text":"Se voc\u00ea deseja avaliar melhor o c\u00f3digo clique aqui , se desejar assistir o v\u00eddeo de explica\u00e7\u00e3o dos resultados clique aqui . Ao ir para o v\u00eddeo me da uma moralzinha, curte, comenta o que voc\u00ea achou, se inscreve no canal e compartilha. Obrigado","title":"Conclus\u00e3o"},{"location":"Projetos/p-7.2-DataQuality/","text":"Data Quality Este projeto de Data Quality foi desenvolvido ap\u00f3s Workshop do Luciano Vasconcelos Filho GitHub do Projeto: Deputados-LuhBorba Documenta\u00e7\u00e3o do Projeto: Documentacao Stack do Projeto python streamlit selenium pytest taskipy pydantic openpyxl mkdocs mkdocstrings mkdocs-material Proposta do Projeto O projeto tem como objetivo realizar um processo de valida\u00e7\u00e3o de estrutura de uma planilha no Excel, considerando que hoje no mundo corporativo esta \u00e9 uma ferramenta amplamente usada, assim buscando defini\u00e7\u00f5es padr\u00f5es para envios de dados considerando um estrutura de contrato pre-definida. A abordagem utilizada \u00e9 criar um App no Streamlit que realize todo processo de valida\u00e7\u00e3o, ferramenta com cria\u00e7\u00e3o de testes, buscando assim um qualidade no dado enviado. Estrutura do Projeto O projeto est\u00e1 basicamente dividido em 4 (quatro) arquivos, todos eles dentro da pasta src . app.py backend.py contrato.py frontend.py App.py Aqui \u00e9 particularmente fa\u00e7o a uni\u00e3o entre o frontend e backend, carregando assim todas as valida\u00e7\u00f5es propostas no backend e trazendo todos os visuais carregados no frontend. # Importando Classe do Arquivo FrontEnd from frontend import ExcelValidadorUI # Importando Fun\u00e7\u00e3o do Arquivo BackEnd from backend import process_excel def main(): ui = ExcelValidadorUI() ui.display_header() upload_file = ui.upload_file() if upload_file: result, error = process_excel(upload_file) ui.display_result(result, error) if __name__ == \"__main__\": main() Frontend.py Este arquivo \u00e9 para configura\u00e7\u00e3o do frontend da p\u00e1gina, assim deixando de forma separada do app.py . # Importando Streamlit import streamlit as st # Criando Classe para FrontEnd, para melhor organiza\u00e7\u00e3o e uso class ExcelValidadorUI: def __init__(self): self.set_page_config() # Criando fun\u00e7\u00e3o para configura\u00e7\u00e3o das p\u00e1ginas def set_page_config(self): st.set_page_config( page_title=\"Validador de Planilha\", page_icon=\":snake:\" ) # Criando fun\u00e7\u00e3o para Header da p\u00e1gina def display_header(self): st.title(\"Insira sua planilha para realizar a valida\u00e7\u00e3o!\") # Criando fun\u00e7\u00e3o para o componente de file uploader def upload_file(self): return st.file_uploader(\"Carregue sua planilha\", type=[\"xlsx\"]) # Criando fun\u00e7\u00e3o especifica para mostragem de error def display_result(self, result, error): if error: st.error(f\"Erro na valida\u00e7\u00e3o: {error}\") else: st.success(\"A planilha est\u00e1 correta\") Backend.py Este arquivo \u00e9 para configura\u00e7\u00e3o do backend da p\u00e1gina, fazendo toda valida\u00e7\u00e3o dos arquivos 'xlsx'. # Importando Pandas import pandas as pd # Importando classe de Vendas do arquivo de contratos.py from contrato import Vendas # Criando fun\u00e7\u00e3o de valida\u00e7\u00e3o do arquivo excel def process_excel(uploaded_file): try: df = pd.read_excel(uploaded_file) # Verificando se h\u00e1 colunas extras extras_cols = set(df.columns) - set(Vendas.model_fields.keys()) if extras_cols: return False, f\"Existem mais colunas que o necess\u00e1rio: {', '.join(extras_cols)}\" # Validar cada linha com o contrato for index, row in df.iterrows(): try: _ = Vendas(**row.to_dict()) except Exception as e: raise ValueError(f\"Erro na linha {index + 2}: {e}\") return True, None except ValueError as ve: return False, str(ve) except Exception as e: return False, f\"Erro Inesperado: {str(e)}\"","title":"Data Quality"},{"location":"Projetos/p-7.2-DataQuality/#data-quality","text":"Este projeto de Data Quality foi desenvolvido ap\u00f3s Workshop do Luciano Vasconcelos Filho GitHub do Projeto: Deputados-LuhBorba Documenta\u00e7\u00e3o do Projeto: Documentacao","title":"Data Quality"},{"location":"Projetos/p-7.2-DataQuality/#stack-do-projeto","text":"python streamlit selenium pytest taskipy pydantic openpyxl mkdocs mkdocstrings mkdocs-material","title":"Stack do Projeto"},{"location":"Projetos/p-7.2-DataQuality/#proposta-do-projeto","text":"O projeto tem como objetivo realizar um processo de valida\u00e7\u00e3o de estrutura de uma planilha no Excel, considerando que hoje no mundo corporativo esta \u00e9 uma ferramenta amplamente usada, assim buscando defini\u00e7\u00f5es padr\u00f5es para envios de dados considerando um estrutura de contrato pre-definida. A abordagem utilizada \u00e9 criar um App no Streamlit que realize todo processo de valida\u00e7\u00e3o, ferramenta com cria\u00e7\u00e3o de testes, buscando assim um qualidade no dado enviado.","title":"Proposta do Projeto"},{"location":"Projetos/p-7.2-DataQuality/#estrutura-do-projeto","text":"O projeto est\u00e1 basicamente dividido em 4 (quatro) arquivos, todos eles dentro da pasta src . app.py backend.py contrato.py frontend.py","title":"Estrutura do Projeto"},{"location":"Projetos/p-7.2-DataQuality/#apppy","text":"Aqui \u00e9 particularmente fa\u00e7o a uni\u00e3o entre o frontend e backend, carregando assim todas as valida\u00e7\u00f5es propostas no backend e trazendo todos os visuais carregados no frontend. # Importando Classe do Arquivo FrontEnd from frontend import ExcelValidadorUI # Importando Fun\u00e7\u00e3o do Arquivo BackEnd from backend import process_excel def main(): ui = ExcelValidadorUI() ui.display_header() upload_file = ui.upload_file() if upload_file: result, error = process_excel(upload_file) ui.display_result(result, error) if __name__ == \"__main__\": main()","title":"App.py"},{"location":"Projetos/p-7.2-DataQuality/#frontendpy","text":"Este arquivo \u00e9 para configura\u00e7\u00e3o do frontend da p\u00e1gina, assim deixando de forma separada do app.py . # Importando Streamlit import streamlit as st # Criando Classe para FrontEnd, para melhor organiza\u00e7\u00e3o e uso class ExcelValidadorUI: def __init__(self): self.set_page_config() # Criando fun\u00e7\u00e3o para configura\u00e7\u00e3o das p\u00e1ginas def set_page_config(self): st.set_page_config( page_title=\"Validador de Planilha\", page_icon=\":snake:\" ) # Criando fun\u00e7\u00e3o para Header da p\u00e1gina def display_header(self): st.title(\"Insira sua planilha para realizar a valida\u00e7\u00e3o!\") # Criando fun\u00e7\u00e3o para o componente de file uploader def upload_file(self): return st.file_uploader(\"Carregue sua planilha\", type=[\"xlsx\"]) # Criando fun\u00e7\u00e3o especifica para mostragem de error def display_result(self, result, error): if error: st.error(f\"Erro na valida\u00e7\u00e3o: {error}\") else: st.success(\"A planilha est\u00e1 correta\")","title":"Frontend.py"},{"location":"Projetos/p-7.2-DataQuality/#backendpy","text":"Este arquivo \u00e9 para configura\u00e7\u00e3o do backend da p\u00e1gina, fazendo toda valida\u00e7\u00e3o dos arquivos 'xlsx'. # Importando Pandas import pandas as pd # Importando classe de Vendas do arquivo de contratos.py from contrato import Vendas # Criando fun\u00e7\u00e3o de valida\u00e7\u00e3o do arquivo excel def process_excel(uploaded_file): try: df = pd.read_excel(uploaded_file) # Verificando se h\u00e1 colunas extras extras_cols = set(df.columns) - set(Vendas.model_fields.keys()) if extras_cols: return False, f\"Existem mais colunas que o necess\u00e1rio: {', '.join(extras_cols)}\" # Validar cada linha com o contrato for index, row in df.iterrows(): try: _ = Vendas(**row.to_dict()) except Exception as e: raise ValueError(f\"Erro na linha {index + 2}: {e}\") return True, None except ValueError as ve: return False, str(ve) except Exception as e: return False, f\"Erro Inesperado: {str(e)}\"","title":"Backend.py"},{"location":"Projetos/p-7.3-Deputados/","text":"Deputados - 1\u00aa Vers\u00e3o [Em Desenvolvimento] GitHub do Projeto: Deputados-LuhBorba Playlist Youtube Codando Passo-a-Passo do Projeto - Youtube Stack do Projeto Python Pandas PySpark Jupyter Notebook Google Colaboratory PGAdmin 4 PostgreSQL Proposta do Projeto A proposta do Projeto foi realizar um pipeline de dados, para posterior analise desses dados. Assim podendo responder a algumas perguntas: Total de Gastos? Valor Total Restitu\u00eddo? M\u00e9dia de Gastos por Deputado? Total Gastos por Estado? Total Gastos por Partido? Propor\u00e7\u00e3o de Gastos de partido por Membro? Top 5 Deputados com mais Gastos? Top 5 Deputados com menos Gastos? Tabela Geral com Informa\u00e7\u00f5es Gerais? Modelagem Dimensional Proposta Neste desenho est\u00e1 faltando a coluna sgPartido, dentro da DIM_DEPUTADOS","title":"Deputados - 1\u00aa Vers\u00e3o"},{"location":"Projetos/p-7.3-Deputados/#deputados-1a-versao","text":"[Em Desenvolvimento] GitHub do Projeto: Deputados-LuhBorba Playlist Youtube Codando Passo-a-Passo do Projeto - Youtube","title":"Deputados - 1\u00aa Vers\u00e3o"},{"location":"Projetos/p-7.3-Deputados/#stack-do-projeto","text":"Python Pandas PySpark Jupyter Notebook Google Colaboratory PGAdmin 4 PostgreSQL","title":"Stack do Projeto"},{"location":"Projetos/p-7.3-Deputados/#proposta-do-projeto","text":"A proposta do Projeto foi realizar um pipeline de dados, para posterior analise desses dados. Assim podendo responder a algumas perguntas: Total de Gastos? Valor Total Restitu\u00eddo? M\u00e9dia de Gastos por Deputado? Total Gastos por Estado? Total Gastos por Partido? Propor\u00e7\u00e3o de Gastos de partido por Membro? Top 5 Deputados com mais Gastos? Top 5 Deputados com menos Gastos? Tabela Geral com Informa\u00e7\u00f5es Gerais?","title":"Proposta do Projeto"},{"location":"Projetos/p-7.3-Deputados/#modelagem-dimensional-proposta","text":"Neste desenho est\u00e1 faltando a coluna sgPartido, dentro da DIM_DEPUTADOS","title":"Modelagem Dimensional Proposta"},{"location":"Projetos/p-7.4-DuckDBePandas/","text":"DuckDB x Pandas Git do Projeto: DuckDb A ideia do projeto \u00e9 gerar dados de acordo com o desejo do usu\u00e1rio, para que possamos simular uma situa\u00e7\u00e3o real de utiliza\u00e7\u00e3o de dados, para tratamento comparando o desempenho do Pandas e DuckDB Regras de Neg\u00f3cio Para esse projeto o Gestor solicitou que fosse extraido o resultado do total de vendas, como tamb\u00e9m total de vendas por produtos e por clientes. Para gerar arquivos nos formatos csv e parquet, destas informa\u00e7\u00f5es para o time de DataViz poder criar incr\u00edveis e infaliveis dashboards. Estrutura Basicamente o projeto cont\u00e9m 2 arquivos python, respons\u00e1veis pela gera\u00e7\u00e3o dos dados, e outros arquivos de ETL com python, usando pandas e duckdb; gerador.py O arquivo gerador.py \u00e9 respons\u00e1vel pela fun\u00e7\u00e3o de gera\u00e7\u00e3o de dados, como tamb\u00e9m de cria\u00e7\u00e3o de pastas para armazenar estes dados. Primeiramente a etapa de importa\u00e7\u00f5es: import csv import os from random import choice from faker import Faker from tqdm import tqdm Criando Estruturas a serem usadas: # Criando Dicion\u00e1rio de Produtos lista_produtos = { \"Computador\": 2000, \"Celular Xiaomi\": 1000, \"Celular Samsung\": 1500, \"Celular Iphone\": 5000, \"Monitor\": 700, \"Teclado\": 50, \"Mouse\": 20, \"HeadSet\": 100, \"HeadSet JBL\": 250, \"Carregador Iphone\": 105, \"Carregador Samsung\": 80, \"Carregador Xiaomi\": 90, \"Carregador Motorola\": 100, \"Carregador Nokia\": 110, \"PenDrive 4 GB\": 20, \"PenDrive 8 GB\": 30, \"PenDrive 16 GB\": 40, } # Criando Lista de Clientes, para ser poss\u00edvel o controle de quantidade de clientes cliente = [] Iniciando Fun\u00e7\u00e3o de gera\u00e7\u00e3o: # Definindo fu\u00e7\u00e3o para gerar dados def gerando_dados( total_linhas: int, total_clientes: int, nome_arquivo: str, tamanho_bloco: int = 100 ): # Invocando a biblioteca faker falso = Faker() # Criando Arquivo with open(nome_arquivo, \"w\", newline=\"\") as file: escrever = csv.writer(file) escrever.writerow( [\"Cliente\", \"Data_Compra\", \"Produto\", \"ValorProduto\", \"Quantidade\"] ) # Definindo Lista aleat\u00f3ria de clientes for _ in range(total_clientes): cliente.append(falso.name()) # Criando Lista para armazenar os dados data = [] # Interando Barra de Progresso considerando o total de linhas, para gerar dados for i in tqdm(range(total_linhas), desc=\"Progresso\", unit=\"linha\"): produto = choice(list(lista_produtos.keys())) data.append( [ choice(cliente), # Escolhendo Cliente falso.date(), # Gerando Data Aleat\u00f3ria produto, # Escolhendo Produto previsto na lista lista_produtos[produto], # Escolhendo Valor falso.random_number(1, 10), # Definindo Quantidade ] ) # Progress\u00e3o dos dados na Barra if i % tamanho_bloco == tamanho_bloco - 1: escrever.writerows(data) data = [] if data: escrever.writerows(data) print(\"Dados gerados com sucesso!\") Agora por fim neste arquivo temos a fun\u00e7\u00e3o respons\u00e1vel pela verifica\u00e7\u00e3o e cria\u00e7\u00e3o das pastas onde ser\u00e1 armazenados os dados: def criar_pastas(diretorio_atual): \"\"\" Cria a estrutura de pastas 'data/csv/' e 'data/parquet/' na pasta atual. Args: diretorio_atual: Caminho da pasta atual. \"\"\" pastas = [\"data/csv/\", \"data/parquet/\"] for pasta in pastas: diretorio_completo = os.path.join(diretorio_atual, pasta) # Verifica se o diret\u00f3rio existe if not os.path.exists(diretorio_completo): # Cria o diret\u00f3rio os.makedirs(diretorio_completo) print(f\"Diret\u00f3rio '{diretorio_completo}' criado com sucesso!\") gerar.py Agora temos tamb\u00e9m o arquivo gerar, respons\u00e1vel apenas para chamar as fun\u00e7\u00f5es do arquivo anterior definindo os parametros: from gerador import gerando_dados, criar_pastas import os gerando_dados(100000000, 20000, \"dados.csv\") # Obt\u00e9m o diret\u00f3rio atual diretorio_atual = os.getcwd() # Cria as pastas criar_pastas(diretorio_atual) teste01-duck-csv.py (Processo de ETL usando Python + DuckDB, para arquivo CSV) Script para realizar ETL dos dados e exportar em CSV, utilizando a biblioteca DuckDB import duckdb import time print(\"Iniciando CSV ...\") # Registrando o tempo de in\u00edcio inicio = time.time() # Conectando ao banco de dados DuckDB con = duckdb.connect() # Carregando dados con.execute(\"CREATE TABLE dados AS SELECT * FROM read_csv('dados.csv');\") # Criando nova coluna de Valor Total con.execute(\"ALTER TABLE dados ADD COLUMN ValorTotal DECIMAL(10,2)\") con.execute(\"UPDATE dados SET ValorTotal = ValorProduto * Quantidade\") # Salvando dados-duck.csv con.execute(\"COPY (SELECT * FROM dados) TO 'data/csv/dados-duck.csv'\") # Salvando total_vendas_produtos-duck.csv con.execute( \"COPY (SELECT Produto, SUM(ValorTotal) AS TotalVendas FROM dados GROUP BY Produto ORDER BY TotalVendas DESC) TO 'data/csv/total_vendas_produtos-duck.csv'\" ) # Salvando total_vendas_cliente-duck.csv con.execute( \"COPY (SELECT Cliente, SUM(ValorTotal) AS TotalVendas FROM dados GROUP BY Cliente ORDER BY TotalVendas DESC) TO 'data/csv/total_vendas_cliente-duck.csv'\" ) # Fechando a conex\u00e3o com o banco de dados con.close() # Registrando o tempo de t\u00e9rmino fim = time.time() # Calculando o tempo de execu\u00e7\u00e3o tempo_execucao = fim - inicio print(\"Finalizado com Sucesso!!!\") print(f\"Tempo de execu\u00e7\u00e3o: {tempo_execucao:.2f} segundos\") teste02-pd-csv.py (Processo de ETL usando Python + Pandas, para arquivo CSV) Script para realizar ETL dos dados e exportar em CSV, utilizando a biblioteca Pandas import pandas as pd import time import os print(\"Iniciando CSV.....!!!\") # Registrando o tempo de in\u00edcio inicio = time.time() # Carregando dados df = pd.read_csv(\"./dados.csv\") # Criando nova coluna de Valor Total df[\"ValorTotal\"] = df[\"ValorProduto\"] * df[\"Quantidade\"] # Verificando o total de vendas por produtos total_vendas_produtos = df.groupby(\"Produto\")[\"ValorTotal\"].sum().reset_index() total_vendas_produtos = total_vendas_produtos.rename( columns={\"ValorTotal\": \"TotalVendas\"} ) total_vendas_produtos = total_vendas_produtos.sort_values( by=\"TotalVendas\", ascending=False ) # Verificando o total de vendas por clientes total_vendas_cliente = df.groupby(\"Cliente\")[\"ValorTotal\"].sum().reset_index() total_vendas_cliente = total_vendas_cliente.rename( columns={\"ValorTotal\": \"TotalVendas\"} ) total_vendas_cliente = total_vendas_cliente.sort_values( by=\"TotalVendas\", ascending=False ) # Salvando DF em CSV df.to_csv(\"data/csv/dados-pandas.csv\", index=False) # Salvando DF de Total De Vendas por Produto total_vendas_produtos.to_csv(\"data/csv/total_vendas_produtos-pandas.csv\", index=False) # Salvando DF de Total De Vendas por Cliente total_vendas_cliente.to_csv(\"data/csv/total_vendas_cliente-pandas.csv\", index=False) # Registrando o tempo de t\u00e9rmino fim = time.time() # Calculando o tempo de execu\u00e7\u00e3o tempo_execucao = fim - inicio print(\"Finalizado com Sucesso!!!\") print(f\"Tempo de execu\u00e7\u00e3o: {tempo_execucao:.2f} segundos\") Resultado Ao utilizar o pandas para o tratamento de dados o mesmo desempenho todas o pipeline de dados em aproximadamente 1.008 segundos (Cerca de 16 minutos), exportando os dados tanto em csv, como tamb\u00e9m em parquet. Conforme imagem abaixo: Ao utilizar o DuckDB, para realizar o mesmo pipeline de dados ele levou 584,83 segundos (Menos de 10 minutos). Conforme imagem abaixo: -- O DuckDB teve uma perfomace 72% melhor do que o Pandas Configura\u00e7\u00e3o do Computador Utilizado Processador: AMD Ryzen 3 5300U with Radeon Graphics 2.6 Ghz RAM: 8 GB DDR-4 SSD Sata: 256 GB SO: Windows 11 23H2 Stack Utilizada no Projeto Python Pandas Faker DuckDB Pyarrow Fastparquet TQDM Taskipy Mkdocs","title":"DuckDB x Pandas"},{"location":"Projetos/p-7.4-DuckDBePandas/#duckdb-x-pandas","text":"Git do Projeto: DuckDb A ideia do projeto \u00e9 gerar dados de acordo com o desejo do usu\u00e1rio, para que possamos simular uma situa\u00e7\u00e3o real de utiliza\u00e7\u00e3o de dados, para tratamento comparando o desempenho do Pandas e DuckDB","title":"DuckDB x Pandas"},{"location":"Projetos/p-7.4-DuckDBePandas/#regras-de-negocio","text":"Para esse projeto o Gestor solicitou que fosse extraido o resultado do total de vendas, como tamb\u00e9m total de vendas por produtos e por clientes. Para gerar arquivos nos formatos csv e parquet, destas informa\u00e7\u00f5es para o time de DataViz poder criar incr\u00edveis e infaliveis dashboards.","title":"Regras de Neg\u00f3cio"},{"location":"Projetos/p-7.4-DuckDBePandas/#estrutura","text":"Basicamente o projeto cont\u00e9m 2 arquivos python, respons\u00e1veis pela gera\u00e7\u00e3o dos dados, e outros arquivos de ETL com python, usando pandas e duckdb;","title":"Estrutura"},{"location":"Projetos/p-7.4-DuckDBePandas/#geradorpy","text":"O arquivo gerador.py \u00e9 respons\u00e1vel pela fun\u00e7\u00e3o de gera\u00e7\u00e3o de dados, como tamb\u00e9m de cria\u00e7\u00e3o de pastas para armazenar estes dados. Primeiramente a etapa de importa\u00e7\u00f5es: import csv import os from random import choice from faker import Faker from tqdm import tqdm Criando Estruturas a serem usadas: # Criando Dicion\u00e1rio de Produtos lista_produtos = { \"Computador\": 2000, \"Celular Xiaomi\": 1000, \"Celular Samsung\": 1500, \"Celular Iphone\": 5000, \"Monitor\": 700, \"Teclado\": 50, \"Mouse\": 20, \"HeadSet\": 100, \"HeadSet JBL\": 250, \"Carregador Iphone\": 105, \"Carregador Samsung\": 80, \"Carregador Xiaomi\": 90, \"Carregador Motorola\": 100, \"Carregador Nokia\": 110, \"PenDrive 4 GB\": 20, \"PenDrive 8 GB\": 30, \"PenDrive 16 GB\": 40, } # Criando Lista de Clientes, para ser poss\u00edvel o controle de quantidade de clientes cliente = [] Iniciando Fun\u00e7\u00e3o de gera\u00e7\u00e3o: # Definindo fu\u00e7\u00e3o para gerar dados def gerando_dados( total_linhas: int, total_clientes: int, nome_arquivo: str, tamanho_bloco: int = 100 ): # Invocando a biblioteca faker falso = Faker() # Criando Arquivo with open(nome_arquivo, \"w\", newline=\"\") as file: escrever = csv.writer(file) escrever.writerow( [\"Cliente\", \"Data_Compra\", \"Produto\", \"ValorProduto\", \"Quantidade\"] ) # Definindo Lista aleat\u00f3ria de clientes for _ in range(total_clientes): cliente.append(falso.name()) # Criando Lista para armazenar os dados data = [] # Interando Barra de Progresso considerando o total de linhas, para gerar dados for i in tqdm(range(total_linhas), desc=\"Progresso\", unit=\"linha\"): produto = choice(list(lista_produtos.keys())) data.append( [ choice(cliente), # Escolhendo Cliente falso.date(), # Gerando Data Aleat\u00f3ria produto, # Escolhendo Produto previsto na lista lista_produtos[produto], # Escolhendo Valor falso.random_number(1, 10), # Definindo Quantidade ] ) # Progress\u00e3o dos dados na Barra if i % tamanho_bloco == tamanho_bloco - 1: escrever.writerows(data) data = [] if data: escrever.writerows(data) print(\"Dados gerados com sucesso!\") Agora por fim neste arquivo temos a fun\u00e7\u00e3o respons\u00e1vel pela verifica\u00e7\u00e3o e cria\u00e7\u00e3o das pastas onde ser\u00e1 armazenados os dados: def criar_pastas(diretorio_atual): \"\"\" Cria a estrutura de pastas 'data/csv/' e 'data/parquet/' na pasta atual. Args: diretorio_atual: Caminho da pasta atual. \"\"\" pastas = [\"data/csv/\", \"data/parquet/\"] for pasta in pastas: diretorio_completo = os.path.join(diretorio_atual, pasta) # Verifica se o diret\u00f3rio existe if not os.path.exists(diretorio_completo): # Cria o diret\u00f3rio os.makedirs(diretorio_completo) print(f\"Diret\u00f3rio '{diretorio_completo}' criado com sucesso!\")","title":"gerador.py"},{"location":"Projetos/p-7.4-DuckDBePandas/#gerarpy","text":"Agora temos tamb\u00e9m o arquivo gerar, respons\u00e1vel apenas para chamar as fun\u00e7\u00f5es do arquivo anterior definindo os parametros: from gerador import gerando_dados, criar_pastas import os gerando_dados(100000000, 20000, \"dados.csv\") # Obt\u00e9m o diret\u00f3rio atual diretorio_atual = os.getcwd() # Cria as pastas criar_pastas(diretorio_atual)","title":"gerar.py"},{"location":"Projetos/p-7.4-DuckDBePandas/#teste01-duck-csvpy-processo-de-etl-usando-python-duckdb-para-arquivo-csv","text":"Script para realizar ETL dos dados e exportar em CSV, utilizando a biblioteca DuckDB import duckdb import time print(\"Iniciando CSV ...\") # Registrando o tempo de in\u00edcio inicio = time.time() # Conectando ao banco de dados DuckDB con = duckdb.connect() # Carregando dados con.execute(\"CREATE TABLE dados AS SELECT * FROM read_csv('dados.csv');\") # Criando nova coluna de Valor Total con.execute(\"ALTER TABLE dados ADD COLUMN ValorTotal DECIMAL(10,2)\") con.execute(\"UPDATE dados SET ValorTotal = ValorProduto * Quantidade\") # Salvando dados-duck.csv con.execute(\"COPY (SELECT * FROM dados) TO 'data/csv/dados-duck.csv'\") # Salvando total_vendas_produtos-duck.csv con.execute( \"COPY (SELECT Produto, SUM(ValorTotal) AS TotalVendas FROM dados GROUP BY Produto ORDER BY TotalVendas DESC) TO 'data/csv/total_vendas_produtos-duck.csv'\" ) # Salvando total_vendas_cliente-duck.csv con.execute( \"COPY (SELECT Cliente, SUM(ValorTotal) AS TotalVendas FROM dados GROUP BY Cliente ORDER BY TotalVendas DESC) TO 'data/csv/total_vendas_cliente-duck.csv'\" ) # Fechando a conex\u00e3o com o banco de dados con.close() # Registrando o tempo de t\u00e9rmino fim = time.time() # Calculando o tempo de execu\u00e7\u00e3o tempo_execucao = fim - inicio print(\"Finalizado com Sucesso!!!\") print(f\"Tempo de execu\u00e7\u00e3o: {tempo_execucao:.2f} segundos\")","title":"teste01-duck-csv.py (Processo de ETL usando Python + DuckDB, para arquivo CSV)"},{"location":"Projetos/p-7.4-DuckDBePandas/#teste02-pd-csvpy-processo-de-etl-usando-python-pandas-para-arquivo-csv","text":"Script para realizar ETL dos dados e exportar em CSV, utilizando a biblioteca Pandas import pandas as pd import time import os print(\"Iniciando CSV.....!!!\") # Registrando o tempo de in\u00edcio inicio = time.time() # Carregando dados df = pd.read_csv(\"./dados.csv\") # Criando nova coluna de Valor Total df[\"ValorTotal\"] = df[\"ValorProduto\"] * df[\"Quantidade\"] # Verificando o total de vendas por produtos total_vendas_produtos = df.groupby(\"Produto\")[\"ValorTotal\"].sum().reset_index() total_vendas_produtos = total_vendas_produtos.rename( columns={\"ValorTotal\": \"TotalVendas\"} ) total_vendas_produtos = total_vendas_produtos.sort_values( by=\"TotalVendas\", ascending=False ) # Verificando o total de vendas por clientes total_vendas_cliente = df.groupby(\"Cliente\")[\"ValorTotal\"].sum().reset_index() total_vendas_cliente = total_vendas_cliente.rename( columns={\"ValorTotal\": \"TotalVendas\"} ) total_vendas_cliente = total_vendas_cliente.sort_values( by=\"TotalVendas\", ascending=False ) # Salvando DF em CSV df.to_csv(\"data/csv/dados-pandas.csv\", index=False) # Salvando DF de Total De Vendas por Produto total_vendas_produtos.to_csv(\"data/csv/total_vendas_produtos-pandas.csv\", index=False) # Salvando DF de Total De Vendas por Cliente total_vendas_cliente.to_csv(\"data/csv/total_vendas_cliente-pandas.csv\", index=False) # Registrando o tempo de t\u00e9rmino fim = time.time() # Calculando o tempo de execu\u00e7\u00e3o tempo_execucao = fim - inicio print(\"Finalizado com Sucesso!!!\") print(f\"Tempo de execu\u00e7\u00e3o: {tempo_execucao:.2f} segundos\")","title":"teste02-pd-csv.py (Processo de ETL usando Python + Pandas, para arquivo CSV)"},{"location":"Projetos/p-7.4-DuckDBePandas/#resultado","text":"Ao utilizar o pandas para o tratamento de dados o mesmo desempenho todas o pipeline de dados em aproximadamente 1.008 segundos (Cerca de 16 minutos), exportando os dados tanto em csv, como tamb\u00e9m em parquet. Conforme imagem abaixo: Ao utilizar o DuckDB, para realizar o mesmo pipeline de dados ele levou 584,83 segundos (Menos de 10 minutos). Conforme imagem abaixo: -- O DuckDB teve uma perfomace 72% melhor do que o Pandas","title":"Resultado"},{"location":"Projetos/p-7.4-DuckDBePandas/#configuracao-do-computador-utilizado","text":"Processador: AMD Ryzen 3 5300U with Radeon Graphics 2.6 Ghz RAM: 8 GB DDR-4 SSD Sata: 256 GB SO: Windows 11 23H2","title":"Configura\u00e7\u00e3o do Computador Utilizado"},{"location":"Projetos/p-7.4-DuckDBePandas/#stack-utilizada-no-projeto","text":"Python Pandas Faker DuckDB Pyarrow Fastparquet TQDM Taskipy Mkdocs","title":"Stack Utilizada no Projeto"}]}